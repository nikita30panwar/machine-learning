{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1.a**"
      ],
      "metadata": {
        "id": "IgQp7ANRVvxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 3 * 32 * 32  # CIFAR-10 images are 32x32x3\n",
        "hidden_size = 512\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, input_size)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'simple_nn.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG13-U7Tyg8C",
        "outputId": "37a2209f-6b5f-4e64-f176-2be5ea56102b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43242474.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Epoch [1/5], Step [100/782], Loss: 2.2687\n",
            "Epoch [1/5], Step [200/782], Loss: 2.2014\n",
            "Epoch [1/5], Step [300/782], Loss: 2.2348\n",
            "Epoch [1/5], Step [400/782], Loss: 2.0980\n",
            "Epoch [1/5], Step [500/782], Loss: 2.1399\n",
            "Epoch [1/5], Step [600/782], Loss: 2.1114\n",
            "Epoch [1/5], Step [700/782], Loss: 2.0175\n",
            "Epoch [2/5], Step [100/782], Loss: 2.0585\n",
            "Epoch [2/5], Step [200/782], Loss: 2.0730\n",
            "Epoch [2/5], Step [300/782], Loss: 2.0765\n",
            "Epoch [2/5], Step [400/782], Loss: 2.0241\n",
            "Epoch [2/5], Step [500/782], Loss: 2.1622\n",
            "Epoch [2/5], Step [600/782], Loss: 2.0549\n",
            "Epoch [2/5], Step [700/782], Loss: 1.9072\n",
            "Epoch [3/5], Step [100/782], Loss: 1.9467\n",
            "Epoch [3/5], Step [200/782], Loss: 1.9146\n",
            "Epoch [3/5], Step [300/782], Loss: 1.9513\n",
            "Epoch [3/5], Step [400/782], Loss: 1.9674\n",
            "Epoch [3/5], Step [500/782], Loss: 1.9518\n",
            "Epoch [3/5], Step [600/782], Loss: 1.8743\n",
            "Epoch [3/5], Step [700/782], Loss: 1.9520\n",
            "Epoch [4/5], Step [100/782], Loss: 1.9006\n",
            "Epoch [4/5], Step [200/782], Loss: 1.8152\n",
            "Epoch [4/5], Step [300/782], Loss: 1.7537\n",
            "Epoch [4/5], Step [400/782], Loss: 1.8408\n",
            "Epoch [4/5], Step [500/782], Loss: 1.9877\n",
            "Epoch [4/5], Step [600/782], Loss: 1.8613\n",
            "Epoch [4/5], Step [700/782], Loss: 1.8617\n",
            "Epoch [5/5], Step [100/782], Loss: 1.8757\n",
            "Epoch [5/5], Step [200/782], Loss: 1.9160\n",
            "Epoch [5/5], Step [300/782], Loss: 1.7423\n",
            "Epoch [5/5], Step [400/782], Loss: 1.8860\n",
            "Epoch [5/5], Step [500/782], Loss: 1.8673\n",
            "Epoch [5/5], Step [600/782], Loss: 1.9153\n",
            "Epoch [5/5], Step [700/782], Loss: 1.8253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.b**"
      ],
      "metadata": {
        "id": "FHU148LMzRZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the neural network with additional hidden layers\n",
        "class ExtendedNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
        "        super(ExtendedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden_size3, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "dBaSt1q40hea"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duiHY_K-0hah",
        "outputId": "60f5fd1e-a28e-45ac-d84d-00bde449a0e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = 3 * 32 * 32  # CIFAR-10 images are 32x32x3\n",
        "hidden_size1 = 512\n",
        "hidden_size2 = 256\n",
        "hidden_size3 = 128\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 300\n"
      ],
      "metadata": {
        "id": "bKkN0WII0hWY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = ExtendedNN(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "0BasG1qb015T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, input_size)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC6-_9X-011u",
        "outputId": "e461d7a5-938b-4e97-aee7-21fa5a356475"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/300], Step [100/782], Loss: 2.3176\n",
            "Epoch [1/300], Step [200/782], Loss: 2.3081\n",
            "Epoch [1/300], Step [300/782], Loss: 2.2963\n",
            "Epoch [1/300], Step [400/782], Loss: 2.2939\n",
            "Epoch [1/300], Step [500/782], Loss: 2.2896\n",
            "Epoch [1/300], Step [600/782], Loss: 2.3007\n",
            "Epoch [1/300], Step [700/782], Loss: 2.3049\n",
            "Epoch [2/300], Step [100/782], Loss: 2.2920\n",
            "Epoch [2/300], Step [200/782], Loss: 2.2980\n",
            "Epoch [2/300], Step [300/782], Loss: 2.2910\n",
            "Epoch [2/300], Step [400/782], Loss: 2.2873\n",
            "Epoch [2/300], Step [500/782], Loss: 2.2974\n",
            "Epoch [2/300], Step [600/782], Loss: 2.2912\n",
            "Epoch [2/300], Step [700/782], Loss: 2.2770\n",
            "Epoch [3/300], Step [100/782], Loss: 2.2779\n",
            "Epoch [3/300], Step [200/782], Loss: 2.2784\n",
            "Epoch [3/300], Step [300/782], Loss: 2.2714\n",
            "Epoch [3/300], Step [400/782], Loss: 2.2729\n",
            "Epoch [3/300], Step [500/782], Loss: 2.2777\n",
            "Epoch [3/300], Step [600/782], Loss: 2.2664\n",
            "Epoch [3/300], Step [700/782], Loss: 2.2750\n",
            "Epoch [4/300], Step [100/782], Loss: 2.2806\n",
            "Epoch [4/300], Step [200/782], Loss: 2.2505\n",
            "Epoch [4/300], Step [300/782], Loss: 2.2701\n",
            "Epoch [4/300], Step [400/782], Loss: 2.2510\n",
            "Epoch [4/300], Step [500/782], Loss: 2.2731\n",
            "Epoch [4/300], Step [600/782], Loss: 2.2352\n",
            "Epoch [4/300], Step [700/782], Loss: 2.2587\n",
            "Epoch [5/300], Step [100/782], Loss: 2.2469\n",
            "Epoch [5/300], Step [200/782], Loss: 2.2251\n",
            "Epoch [5/300], Step [300/782], Loss: 2.2405\n",
            "Epoch [5/300], Step [400/782], Loss: 2.2214\n",
            "Epoch [5/300], Step [500/782], Loss: 2.2178\n",
            "Epoch [5/300], Step [600/782], Loss: 2.2278\n",
            "Epoch [5/300], Step [700/782], Loss: 2.2222\n",
            "Epoch [6/300], Step [100/782], Loss: 2.1723\n",
            "Epoch [6/300], Step [200/782], Loss: 2.2391\n",
            "Epoch [6/300], Step [300/782], Loss: 2.2027\n",
            "Epoch [6/300], Step [400/782], Loss: 2.2097\n",
            "Epoch [6/300], Step [500/782], Loss: 2.1646\n",
            "Epoch [6/300], Step [600/782], Loss: 2.1799\n",
            "Epoch [6/300], Step [700/782], Loss: 2.2371\n",
            "Epoch [7/300], Step [100/782], Loss: 2.1932\n",
            "Epoch [7/300], Step [200/782], Loss: 2.1924\n",
            "Epoch [7/300], Step [300/782], Loss: 2.1538\n",
            "Epoch [7/300], Step [400/782], Loss: 2.1365\n",
            "Epoch [7/300], Step [500/782], Loss: 2.1652\n",
            "Epoch [7/300], Step [600/782], Loss: 2.1728\n",
            "Epoch [7/300], Step [700/782], Loss: 2.1124\n",
            "Epoch [8/300], Step [100/782], Loss: 2.1490\n",
            "Epoch [8/300], Step [200/782], Loss: 2.2141\n",
            "Epoch [8/300], Step [300/782], Loss: 2.1373\n",
            "Epoch [8/300], Step [400/782], Loss: 2.1139\n",
            "Epoch [8/300], Step [500/782], Loss: 2.0296\n",
            "Epoch [8/300], Step [600/782], Loss: 2.1856\n",
            "Epoch [8/300], Step [700/782], Loss: 2.1265\n",
            "Epoch [9/300], Step [100/782], Loss: 2.2056\n",
            "Epoch [9/300], Step [200/782], Loss: 2.2028\n",
            "Epoch [9/300], Step [300/782], Loss: 2.0462\n",
            "Epoch [9/300], Step [400/782], Loss: 2.0287\n",
            "Epoch [9/300], Step [500/782], Loss: 2.0935\n",
            "Epoch [9/300], Step [600/782], Loss: 2.0726\n",
            "Epoch [9/300], Step [700/782], Loss: 2.1128\n",
            "Epoch [10/300], Step [100/782], Loss: 2.0818\n",
            "Epoch [10/300], Step [200/782], Loss: 2.0186\n",
            "Epoch [10/300], Step [300/782], Loss: 1.9662\n",
            "Epoch [10/300], Step [400/782], Loss: 2.0420\n",
            "Epoch [10/300], Step [500/782], Loss: 2.0969\n",
            "Epoch [10/300], Step [600/782], Loss: 2.0512\n",
            "Epoch [10/300], Step [700/782], Loss: 2.0782\n",
            "Epoch [11/300], Step [100/782], Loss: 1.9787\n",
            "Epoch [11/300], Step [200/782], Loss: 1.9096\n",
            "Epoch [11/300], Step [300/782], Loss: 1.8656\n",
            "Epoch [11/300], Step [400/782], Loss: 2.0420\n",
            "Epoch [11/300], Step [500/782], Loss: 2.0155\n",
            "Epoch [11/300], Step [600/782], Loss: 1.9983\n",
            "Epoch [11/300], Step [700/782], Loss: 2.0583\n",
            "Epoch [12/300], Step [100/782], Loss: 1.9852\n",
            "Epoch [12/300], Step [200/782], Loss: 2.0262\n",
            "Epoch [12/300], Step [300/782], Loss: 1.8839\n",
            "Epoch [12/300], Step [400/782], Loss: 1.8867\n",
            "Epoch [12/300], Step [500/782], Loss: 2.0093\n",
            "Epoch [12/300], Step [600/782], Loss: 2.0545\n",
            "Epoch [12/300], Step [700/782], Loss: 1.9030\n",
            "Epoch [13/300], Step [100/782], Loss: 1.8521\n",
            "Epoch [13/300], Step [200/782], Loss: 2.0533\n",
            "Epoch [13/300], Step [300/782], Loss: 2.0070\n",
            "Epoch [13/300], Step [400/782], Loss: 1.9426\n",
            "Epoch [13/300], Step [500/782], Loss: 2.0077\n",
            "Epoch [13/300], Step [600/782], Loss: 1.8736\n",
            "Epoch [13/300], Step [700/782], Loss: 1.9652\n",
            "Epoch [14/300], Step [100/782], Loss: 1.8762\n",
            "Epoch [14/300], Step [200/782], Loss: 1.9193\n",
            "Epoch [14/300], Step [300/782], Loss: 1.9856\n",
            "Epoch [14/300], Step [400/782], Loss: 2.0326\n",
            "Epoch [14/300], Step [500/782], Loss: 1.9081\n",
            "Epoch [14/300], Step [600/782], Loss: 1.9450\n",
            "Epoch [14/300], Step [700/782], Loss: 2.1295\n",
            "Epoch [15/300], Step [100/782], Loss: 1.8599\n",
            "Epoch [15/300], Step [200/782], Loss: 1.8904\n",
            "Epoch [15/300], Step [300/782], Loss: 1.8191\n",
            "Epoch [15/300], Step [400/782], Loss: 1.8024\n",
            "Epoch [15/300], Step [500/782], Loss: 1.9398\n",
            "Epoch [15/300], Step [600/782], Loss: 2.0805\n",
            "Epoch [15/300], Step [700/782], Loss: 2.2497\n",
            "Epoch [16/300], Step [100/782], Loss: 1.8852\n",
            "Epoch [16/300], Step [200/782], Loss: 1.8060\n",
            "Epoch [16/300], Step [300/782], Loss: 1.9872\n",
            "Epoch [16/300], Step [400/782], Loss: 1.9687\n",
            "Epoch [16/300], Step [500/782], Loss: 1.8054\n",
            "Epoch [16/300], Step [600/782], Loss: 1.9650\n",
            "Epoch [16/300], Step [700/782], Loss: 2.0459\n",
            "Epoch [17/300], Step [100/782], Loss: 1.9314\n",
            "Epoch [17/300], Step [200/782], Loss: 1.8496\n",
            "Epoch [17/300], Step [300/782], Loss: 1.9580\n",
            "Epoch [17/300], Step [400/782], Loss: 1.9248\n",
            "Epoch [17/300], Step [500/782], Loss: 1.7724\n",
            "Epoch [17/300], Step [600/782], Loss: 1.8933\n",
            "Epoch [17/300], Step [700/782], Loss: 1.9141\n",
            "Epoch [18/300], Step [100/782], Loss: 1.9082\n",
            "Epoch [18/300], Step [200/782], Loss: 1.9615\n",
            "Epoch [18/300], Step [300/782], Loss: 1.7325\n",
            "Epoch [18/300], Step [400/782], Loss: 1.6641\n",
            "Epoch [18/300], Step [500/782], Loss: 1.9380\n",
            "Epoch [18/300], Step [600/782], Loss: 1.9168\n",
            "Epoch [18/300], Step [700/782], Loss: 1.8751\n",
            "Epoch [19/300], Step [100/782], Loss: 1.8516\n",
            "Epoch [19/300], Step [200/782], Loss: 1.8935\n",
            "Epoch [19/300], Step [300/782], Loss: 1.7991\n",
            "Epoch [19/300], Step [400/782], Loss: 1.8571\n",
            "Epoch [19/300], Step [500/782], Loss: 1.6687\n",
            "Epoch [19/300], Step [600/782], Loss: 1.7785\n",
            "Epoch [19/300], Step [700/782], Loss: 1.8574\n",
            "Epoch [20/300], Step [100/782], Loss: 2.0363\n",
            "Epoch [20/300], Step [200/782], Loss: 1.6812\n",
            "Epoch [20/300], Step [300/782], Loss: 1.7491\n",
            "Epoch [20/300], Step [400/782], Loss: 1.7088\n",
            "Epoch [20/300], Step [500/782], Loss: 1.6178\n",
            "Epoch [20/300], Step [600/782], Loss: 1.9521\n",
            "Epoch [20/300], Step [700/782], Loss: 1.7567\n",
            "Epoch [21/300], Step [100/782], Loss: 1.8320\n",
            "Epoch [21/300], Step [200/782], Loss: 1.8253\n",
            "Epoch [21/300], Step [300/782], Loss: 1.6675\n",
            "Epoch [21/300], Step [400/782], Loss: 1.6447\n",
            "Epoch [21/300], Step [500/782], Loss: 1.8073\n",
            "Epoch [21/300], Step [600/782], Loss: 1.8415\n",
            "Epoch [21/300], Step [700/782], Loss: 1.9996\n",
            "Epoch [22/300], Step [100/782], Loss: 1.8219\n",
            "Epoch [22/300], Step [200/782], Loss: 1.8238\n",
            "Epoch [22/300], Step [300/782], Loss: 1.9951\n",
            "Epoch [22/300], Step [400/782], Loss: 1.7255\n",
            "Epoch [22/300], Step [500/782], Loss: 1.6236\n",
            "Epoch [22/300], Step [600/782], Loss: 1.7008\n",
            "Epoch [22/300], Step [700/782], Loss: 1.6317\n",
            "Epoch [23/300], Step [100/782], Loss: 1.8433\n",
            "Epoch [23/300], Step [200/782], Loss: 1.7589\n",
            "Epoch [23/300], Step [300/782], Loss: 1.8628\n",
            "Epoch [23/300], Step [400/782], Loss: 1.8970\n",
            "Epoch [23/300], Step [500/782], Loss: 1.8852\n",
            "Epoch [23/300], Step [600/782], Loss: 1.6879\n",
            "Epoch [23/300], Step [700/782], Loss: 1.7231\n",
            "Epoch [24/300], Step [100/782], Loss: 1.8351\n",
            "Epoch [24/300], Step [200/782], Loss: 1.6120\n",
            "Epoch [24/300], Step [300/782], Loss: 1.5828\n",
            "Epoch [24/300], Step [400/782], Loss: 1.7214\n",
            "Epoch [24/300], Step [500/782], Loss: 1.5006\n",
            "Epoch [24/300], Step [600/782], Loss: 1.8197\n",
            "Epoch [24/300], Step [700/782], Loss: 2.0360\n",
            "Epoch [25/300], Step [100/782], Loss: 1.8447\n",
            "Epoch [25/300], Step [200/782], Loss: 1.7119\n",
            "Epoch [25/300], Step [300/782], Loss: 1.7834\n",
            "Epoch [25/300], Step [400/782], Loss: 1.7830\n",
            "Epoch [25/300], Step [500/782], Loss: 1.6255\n",
            "Epoch [25/300], Step [600/782], Loss: 1.7030\n",
            "Epoch [25/300], Step [700/782], Loss: 1.7729\n",
            "Epoch [26/300], Step [100/782], Loss: 1.5735\n",
            "Epoch [26/300], Step [200/782], Loss: 1.4897\n",
            "Epoch [26/300], Step [300/782], Loss: 1.8191\n",
            "Epoch [26/300], Step [400/782], Loss: 1.7577\n",
            "Epoch [26/300], Step [500/782], Loss: 1.6218\n",
            "Epoch [26/300], Step [600/782], Loss: 1.8735\n",
            "Epoch [26/300], Step [700/782], Loss: 1.7463\n",
            "Epoch [27/300], Step [100/782], Loss: 1.5863\n",
            "Epoch [27/300], Step [200/782], Loss: 1.7716\n",
            "Epoch [27/300], Step [300/782], Loss: 1.6457\n",
            "Epoch [27/300], Step [400/782], Loss: 1.6964\n",
            "Epoch [27/300], Step [500/782], Loss: 1.6316\n",
            "Epoch [27/300], Step [600/782], Loss: 1.7213\n",
            "Epoch [27/300], Step [700/782], Loss: 1.7216\n",
            "Epoch [28/300], Step [100/782], Loss: 1.7881\n",
            "Epoch [28/300], Step [200/782], Loss: 1.5511\n",
            "Epoch [28/300], Step [300/782], Loss: 1.6959\n",
            "Epoch [28/300], Step [400/782], Loss: 1.5655\n",
            "Epoch [28/300], Step [500/782], Loss: 1.6530\n",
            "Epoch [28/300], Step [600/782], Loss: 1.6019\n",
            "Epoch [28/300], Step [700/782], Loss: 1.7630\n",
            "Epoch [29/300], Step [100/782], Loss: 1.9485\n",
            "Epoch [29/300], Step [200/782], Loss: 1.4789\n",
            "Epoch [29/300], Step [300/782], Loss: 1.4891\n",
            "Epoch [29/300], Step [400/782], Loss: 1.6136\n",
            "Epoch [29/300], Step [500/782], Loss: 1.7874\n",
            "Epoch [29/300], Step [600/782], Loss: 1.5972\n",
            "Epoch [29/300], Step [700/782], Loss: 1.6869\n",
            "Epoch [30/300], Step [100/782], Loss: 1.7769\n",
            "Epoch [30/300], Step [200/782], Loss: 1.7264\n",
            "Epoch [30/300], Step [300/782], Loss: 1.4014\n",
            "Epoch [30/300], Step [400/782], Loss: 1.6762\n",
            "Epoch [30/300], Step [500/782], Loss: 1.5584\n",
            "Epoch [30/300], Step [600/782], Loss: 1.6908\n",
            "Epoch [30/300], Step [700/782], Loss: 1.7267\n",
            "Epoch [31/300], Step [100/782], Loss: 1.5833\n",
            "Epoch [31/300], Step [200/782], Loss: 1.5651\n",
            "Epoch [31/300], Step [300/782], Loss: 1.5214\n",
            "Epoch [31/300], Step [400/782], Loss: 1.6454\n",
            "Epoch [31/300], Step [500/782], Loss: 1.7242\n",
            "Epoch [31/300], Step [600/782], Loss: 1.6343\n",
            "Epoch [31/300], Step [700/782], Loss: 1.8047\n",
            "Epoch [32/300], Step [100/782], Loss: 1.6552\n",
            "Epoch [32/300], Step [200/782], Loss: 1.7410\n",
            "Epoch [32/300], Step [300/782], Loss: 1.6674\n",
            "Epoch [32/300], Step [400/782], Loss: 1.5856\n",
            "Epoch [32/300], Step [500/782], Loss: 1.5299\n",
            "Epoch [32/300], Step [600/782], Loss: 1.5756\n",
            "Epoch [32/300], Step [700/782], Loss: 1.5749\n",
            "Epoch [33/300], Step [100/782], Loss: 1.6520\n",
            "Epoch [33/300], Step [200/782], Loss: 1.6875\n",
            "Epoch [33/300], Step [300/782], Loss: 1.7971\n",
            "Epoch [33/300], Step [400/782], Loss: 1.6327\n",
            "Epoch [33/300], Step [500/782], Loss: 1.8233\n",
            "Epoch [33/300], Step [600/782], Loss: 1.7960\n",
            "Epoch [33/300], Step [700/782], Loss: 1.8118\n",
            "Epoch [34/300], Step [100/782], Loss: 1.5353\n",
            "Epoch [34/300], Step [200/782], Loss: 1.6118\n",
            "Epoch [34/300], Step [300/782], Loss: 1.8620\n",
            "Epoch [34/300], Step [400/782], Loss: 1.8110\n",
            "Epoch [34/300], Step [500/782], Loss: 1.5502\n",
            "Epoch [34/300], Step [600/782], Loss: 1.5269\n",
            "Epoch [34/300], Step [700/782], Loss: 1.6312\n",
            "Epoch [35/300], Step [100/782], Loss: 1.6801\n",
            "Epoch [35/300], Step [200/782], Loss: 1.6520\n",
            "Epoch [35/300], Step [300/782], Loss: 1.7052\n",
            "Epoch [35/300], Step [400/782], Loss: 1.6225\n",
            "Epoch [35/300], Step [500/782], Loss: 1.7106\n",
            "Epoch [35/300], Step [600/782], Loss: 1.7875\n",
            "Epoch [35/300], Step [700/782], Loss: 1.5948\n",
            "Epoch [36/300], Step [100/782], Loss: 1.5755\n",
            "Epoch [36/300], Step [200/782], Loss: 1.3006\n",
            "Epoch [36/300], Step [300/782], Loss: 1.7132\n",
            "Epoch [36/300], Step [400/782], Loss: 1.6011\n",
            "Epoch [36/300], Step [500/782], Loss: 1.7642\n",
            "Epoch [36/300], Step [600/782], Loss: 1.7340\n",
            "Epoch [36/300], Step [700/782], Loss: 1.5170\n",
            "Epoch [37/300], Step [100/782], Loss: 1.6479\n",
            "Epoch [37/300], Step [200/782], Loss: 1.5912\n",
            "Epoch [37/300], Step [300/782], Loss: 1.6590\n",
            "Epoch [37/300], Step [400/782], Loss: 1.4699\n",
            "Epoch [37/300], Step [500/782], Loss: 1.6241\n",
            "Epoch [37/300], Step [600/782], Loss: 1.5572\n",
            "Epoch [37/300], Step [700/782], Loss: 1.9246\n",
            "Epoch [38/300], Step [100/782], Loss: 1.8621\n",
            "Epoch [38/300], Step [200/782], Loss: 1.6652\n",
            "Epoch [38/300], Step [300/782], Loss: 1.7242\n",
            "Epoch [38/300], Step [400/782], Loss: 1.4410\n",
            "Epoch [38/300], Step [500/782], Loss: 1.5913\n",
            "Epoch [38/300], Step [600/782], Loss: 1.4853\n",
            "Epoch [38/300], Step [700/782], Loss: 1.4676\n",
            "Epoch [39/300], Step [100/782], Loss: 1.6260\n",
            "Epoch [39/300], Step [200/782], Loss: 1.6110\n",
            "Epoch [39/300], Step [300/782], Loss: 1.6277\n",
            "Epoch [39/300], Step [400/782], Loss: 1.7296\n",
            "Epoch [39/300], Step [500/782], Loss: 1.6383\n",
            "Epoch [39/300], Step [600/782], Loss: 1.7744\n",
            "Epoch [39/300], Step [700/782], Loss: 1.6064\n",
            "Epoch [40/300], Step [100/782], Loss: 1.7494\n",
            "Epoch [40/300], Step [200/782], Loss: 1.4580\n",
            "Epoch [40/300], Step [300/782], Loss: 1.5874\n",
            "Epoch [40/300], Step [400/782], Loss: 1.6991\n",
            "Epoch [40/300], Step [500/782], Loss: 1.6134\n",
            "Epoch [40/300], Step [600/782], Loss: 1.8372\n",
            "Epoch [40/300], Step [700/782], Loss: 1.3247\n",
            "Epoch [41/300], Step [100/782], Loss: 1.5684\n",
            "Epoch [41/300], Step [200/782], Loss: 1.5156\n",
            "Epoch [41/300], Step [300/782], Loss: 1.6516\n",
            "Epoch [41/300], Step [400/782], Loss: 1.5702\n",
            "Epoch [41/300], Step [500/782], Loss: 1.8421\n",
            "Epoch [41/300], Step [600/782], Loss: 1.7318\n",
            "Epoch [41/300], Step [700/782], Loss: 1.5460\n",
            "Epoch [42/300], Step [100/782], Loss: 1.7002\n",
            "Epoch [42/300], Step [200/782], Loss: 1.5596\n",
            "Epoch [42/300], Step [300/782], Loss: 1.4814\n",
            "Epoch [42/300], Step [400/782], Loss: 1.5254\n",
            "Epoch [42/300], Step [500/782], Loss: 1.6255\n",
            "Epoch [42/300], Step [600/782], Loss: 1.5992\n",
            "Epoch [42/300], Step [700/782], Loss: 1.5251\n",
            "Epoch [43/300], Step [100/782], Loss: 1.6443\n",
            "Epoch [43/300], Step [200/782], Loss: 1.4763\n",
            "Epoch [43/300], Step [300/782], Loss: 1.4621\n",
            "Epoch [43/300], Step [400/782], Loss: 1.5984\n",
            "Epoch [43/300], Step [500/782], Loss: 1.4261\n",
            "Epoch [43/300], Step [600/782], Loss: 1.4989\n",
            "Epoch [43/300], Step [700/782], Loss: 1.5775\n",
            "Epoch [44/300], Step [100/782], Loss: 1.6821\n",
            "Epoch [44/300], Step [200/782], Loss: 1.4934\n",
            "Epoch [44/300], Step [300/782], Loss: 1.4489\n",
            "Epoch [44/300], Step [400/782], Loss: 1.5042\n",
            "Epoch [44/300], Step [500/782], Loss: 1.6596\n",
            "Epoch [44/300], Step [600/782], Loss: 1.4685\n",
            "Epoch [44/300], Step [700/782], Loss: 1.5906\n",
            "Epoch [45/300], Step [100/782], Loss: 1.3743\n",
            "Epoch [45/300], Step [200/782], Loss: 1.4857\n",
            "Epoch [45/300], Step [300/782], Loss: 1.4064\n",
            "Epoch [45/300], Step [400/782], Loss: 1.4846\n",
            "Epoch [45/300], Step [500/782], Loss: 1.4447\n",
            "Epoch [45/300], Step [600/782], Loss: 1.5547\n",
            "Epoch [45/300], Step [700/782], Loss: 1.4678\n",
            "Epoch [46/300], Step [100/782], Loss: 1.7086\n",
            "Epoch [46/300], Step [200/782], Loss: 1.5707\n",
            "Epoch [46/300], Step [300/782], Loss: 1.4559\n",
            "Epoch [46/300], Step [400/782], Loss: 1.7454\n",
            "Epoch [46/300], Step [500/782], Loss: 1.4817\n",
            "Epoch [46/300], Step [600/782], Loss: 1.7029\n",
            "Epoch [46/300], Step [700/782], Loss: 1.4226\n",
            "Epoch [47/300], Step [100/782], Loss: 1.4782\n",
            "Epoch [47/300], Step [200/782], Loss: 1.6253\n",
            "Epoch [47/300], Step [300/782], Loss: 1.5249\n",
            "Epoch [47/300], Step [400/782], Loss: 1.7050\n",
            "Epoch [47/300], Step [500/782], Loss: 1.4577\n",
            "Epoch [47/300], Step [600/782], Loss: 1.3769\n",
            "Epoch [47/300], Step [700/782], Loss: 1.6635\n",
            "Epoch [48/300], Step [100/782], Loss: 1.4490\n",
            "Epoch [48/300], Step [200/782], Loss: 1.5288\n",
            "Epoch [48/300], Step [300/782], Loss: 1.5303\n",
            "Epoch [48/300], Step [400/782], Loss: 1.5079\n",
            "Epoch [48/300], Step [500/782], Loss: 1.3881\n",
            "Epoch [48/300], Step [600/782], Loss: 1.3704\n",
            "Epoch [48/300], Step [700/782], Loss: 1.3732\n",
            "Epoch [49/300], Step [100/782], Loss: 1.6438\n",
            "Epoch [49/300], Step [200/782], Loss: 1.6524\n",
            "Epoch [49/300], Step [300/782], Loss: 1.5522\n",
            "Epoch [49/300], Step [400/782], Loss: 1.5305\n",
            "Epoch [49/300], Step [500/782], Loss: 1.3724\n",
            "Epoch [49/300], Step [600/782], Loss: 1.4840\n",
            "Epoch [49/300], Step [700/782], Loss: 1.3779\n",
            "Epoch [50/300], Step [100/782], Loss: 1.5663\n",
            "Epoch [50/300], Step [200/782], Loss: 1.3040\n",
            "Epoch [50/300], Step [300/782], Loss: 1.4797\n",
            "Epoch [50/300], Step [400/782], Loss: 1.5715\n",
            "Epoch [50/300], Step [500/782], Loss: 1.5330\n",
            "Epoch [50/300], Step [600/782], Loss: 1.5452\n",
            "Epoch [50/300], Step [700/782], Loss: 1.5142\n",
            "Epoch [51/300], Step [100/782], Loss: 1.3584\n",
            "Epoch [51/300], Step [200/782], Loss: 1.5941\n",
            "Epoch [51/300], Step [300/782], Loss: 1.4266\n",
            "Epoch [51/300], Step [400/782], Loss: 1.5254\n",
            "Epoch [51/300], Step [500/782], Loss: 1.4357\n",
            "Epoch [51/300], Step [600/782], Loss: 1.4583\n",
            "Epoch [51/300], Step [700/782], Loss: 1.4080\n",
            "Epoch [52/300], Step [100/782], Loss: 1.3840\n",
            "Epoch [52/300], Step [200/782], Loss: 1.5583\n",
            "Epoch [52/300], Step [300/782], Loss: 1.4685\n",
            "Epoch [52/300], Step [400/782], Loss: 1.4151\n",
            "Epoch [52/300], Step [500/782], Loss: 1.6952\n",
            "Epoch [52/300], Step [600/782], Loss: 1.4454\n",
            "Epoch [52/300], Step [700/782], Loss: 1.6279\n",
            "Epoch [53/300], Step [100/782], Loss: 1.3375\n",
            "Epoch [53/300], Step [200/782], Loss: 1.5350\n",
            "Epoch [53/300], Step [300/782], Loss: 1.4039\n",
            "Epoch [53/300], Step [400/782], Loss: 1.5519\n",
            "Epoch [53/300], Step [500/782], Loss: 1.5003\n",
            "Epoch [53/300], Step [600/782], Loss: 1.5344\n",
            "Epoch [53/300], Step [700/782], Loss: 1.4631\n",
            "Epoch [54/300], Step [100/782], Loss: 1.4191\n",
            "Epoch [54/300], Step [200/782], Loss: 1.3469\n",
            "Epoch [54/300], Step [300/782], Loss: 1.4013\n",
            "Epoch [54/300], Step [400/782], Loss: 1.7845\n",
            "Epoch [54/300], Step [500/782], Loss: 1.4267\n",
            "Epoch [54/300], Step [600/782], Loss: 1.5962\n",
            "Epoch [54/300], Step [700/782], Loss: 1.5002\n",
            "Epoch [55/300], Step [100/782], Loss: 1.5268\n",
            "Epoch [55/300], Step [200/782], Loss: 1.3903\n",
            "Epoch [55/300], Step [300/782], Loss: 1.6512\n",
            "Epoch [55/300], Step [400/782], Loss: 1.6281\n",
            "Epoch [55/300], Step [500/782], Loss: 1.5828\n",
            "Epoch [55/300], Step [600/782], Loss: 1.5419\n",
            "Epoch [55/300], Step [700/782], Loss: 1.4751\n",
            "Epoch [56/300], Step [100/782], Loss: 1.5856\n",
            "Epoch [56/300], Step [200/782], Loss: 1.3411\n",
            "Epoch [56/300], Step [300/782], Loss: 1.4994\n",
            "Epoch [56/300], Step [400/782], Loss: 1.5866\n",
            "Epoch [56/300], Step [500/782], Loss: 1.5261\n",
            "Epoch [56/300], Step [600/782], Loss: 1.3961\n",
            "Epoch [56/300], Step [700/782], Loss: 1.4053\n",
            "Epoch [57/300], Step [100/782], Loss: 1.6561\n",
            "Epoch [57/300], Step [200/782], Loss: 1.3598\n",
            "Epoch [57/300], Step [300/782], Loss: 1.4301\n",
            "Epoch [57/300], Step [400/782], Loss: 1.3857\n",
            "Epoch [57/300], Step [500/782], Loss: 1.5349\n",
            "Epoch [57/300], Step [600/782], Loss: 1.4895\n",
            "Epoch [57/300], Step [700/782], Loss: 1.4255\n",
            "Epoch [58/300], Step [100/782], Loss: 1.4415\n",
            "Epoch [58/300], Step [200/782], Loss: 1.5325\n",
            "Epoch [58/300], Step [300/782], Loss: 1.5629\n",
            "Epoch [58/300], Step [400/782], Loss: 1.4443\n",
            "Epoch [58/300], Step [500/782], Loss: 1.2920\n",
            "Epoch [58/300], Step [600/782], Loss: 1.2548\n",
            "Epoch [58/300], Step [700/782], Loss: 1.7001\n",
            "Epoch [59/300], Step [100/782], Loss: 1.3956\n",
            "Epoch [59/300], Step [200/782], Loss: 1.2923\n",
            "Epoch [59/300], Step [300/782], Loss: 1.3118\n",
            "Epoch [59/300], Step [400/782], Loss: 1.4221\n",
            "Epoch [59/300], Step [500/782], Loss: 1.4827\n",
            "Epoch [59/300], Step [600/782], Loss: 1.4552\n",
            "Epoch [59/300], Step [700/782], Loss: 1.3670\n",
            "Epoch [60/300], Step [100/782], Loss: 1.5606\n",
            "Epoch [60/300], Step [200/782], Loss: 1.4150\n",
            "Epoch [60/300], Step [300/782], Loss: 1.3892\n",
            "Epoch [60/300], Step [400/782], Loss: 1.3397\n",
            "Epoch [60/300], Step [500/782], Loss: 1.4096\n",
            "Epoch [60/300], Step [600/782], Loss: 1.5324\n",
            "Epoch [60/300], Step [700/782], Loss: 1.3862\n",
            "Epoch [61/300], Step [100/782], Loss: 1.5411\n",
            "Epoch [61/300], Step [200/782], Loss: 1.4254\n",
            "Epoch [61/300], Step [300/782], Loss: 1.4483\n",
            "Epoch [61/300], Step [400/782], Loss: 1.2952\n",
            "Epoch [61/300], Step [500/782], Loss: 1.3533\n",
            "Epoch [61/300], Step [600/782], Loss: 1.4177\n",
            "Epoch [61/300], Step [700/782], Loss: 1.4391\n",
            "Epoch [62/300], Step [100/782], Loss: 1.4284\n",
            "Epoch [62/300], Step [200/782], Loss: 1.4449\n",
            "Epoch [62/300], Step [300/782], Loss: 1.4533\n",
            "Epoch [62/300], Step [400/782], Loss: 1.4781\n",
            "Epoch [62/300], Step [500/782], Loss: 1.4154\n",
            "Epoch [62/300], Step [600/782], Loss: 1.3640\n",
            "Epoch [62/300], Step [700/782], Loss: 1.4084\n",
            "Epoch [63/300], Step [100/782], Loss: 1.3838\n",
            "Epoch [63/300], Step [200/782], Loss: 1.3148\n",
            "Epoch [63/300], Step [300/782], Loss: 1.0963\n",
            "Epoch [63/300], Step [400/782], Loss: 1.4249\n",
            "Epoch [63/300], Step [500/782], Loss: 1.5160\n",
            "Epoch [63/300], Step [600/782], Loss: 1.3717\n",
            "Epoch [63/300], Step [700/782], Loss: 1.3548\n",
            "Epoch [64/300], Step [100/782], Loss: 1.5193\n",
            "Epoch [64/300], Step [200/782], Loss: 1.3405\n",
            "Epoch [64/300], Step [300/782], Loss: 1.2653\n",
            "Epoch [64/300], Step [400/782], Loss: 1.4439\n",
            "Epoch [64/300], Step [500/782], Loss: 1.4092\n",
            "Epoch [64/300], Step [600/782], Loss: 1.3893\n",
            "Epoch [64/300], Step [700/782], Loss: 1.3882\n",
            "Epoch [65/300], Step [100/782], Loss: 1.3004\n",
            "Epoch [65/300], Step [200/782], Loss: 1.5066\n",
            "Epoch [65/300], Step [300/782], Loss: 1.1271\n",
            "Epoch [65/300], Step [400/782], Loss: 1.6593\n",
            "Epoch [65/300], Step [500/782], Loss: 1.4318\n",
            "Epoch [65/300], Step [600/782], Loss: 1.2592\n",
            "Epoch [65/300], Step [700/782], Loss: 1.5082\n",
            "Epoch [66/300], Step [100/782], Loss: 1.4738\n",
            "Epoch [66/300], Step [200/782], Loss: 1.3372\n",
            "Epoch [66/300], Step [300/782], Loss: 1.7133\n",
            "Epoch [66/300], Step [400/782], Loss: 1.4482\n",
            "Epoch [66/300], Step [500/782], Loss: 1.4119\n",
            "Epoch [66/300], Step [600/782], Loss: 1.4231\n",
            "Epoch [66/300], Step [700/782], Loss: 1.3405\n",
            "Epoch [67/300], Step [100/782], Loss: 1.5159\n",
            "Epoch [67/300], Step [200/782], Loss: 1.3870\n",
            "Epoch [67/300], Step [300/782], Loss: 1.3578\n",
            "Epoch [67/300], Step [400/782], Loss: 1.2812\n",
            "Epoch [67/300], Step [500/782], Loss: 1.1610\n",
            "Epoch [67/300], Step [600/782], Loss: 1.2728\n",
            "Epoch [67/300], Step [700/782], Loss: 1.3293\n",
            "Epoch [68/300], Step [100/782], Loss: 1.4409\n",
            "Epoch [68/300], Step [200/782], Loss: 1.4139\n",
            "Epoch [68/300], Step [300/782], Loss: 1.2052\n",
            "Epoch [68/300], Step [400/782], Loss: 1.3677\n",
            "Epoch [68/300], Step [500/782], Loss: 1.4442\n",
            "Epoch [68/300], Step [600/782], Loss: 1.3972\n",
            "Epoch [68/300], Step [700/782], Loss: 1.3887\n",
            "Epoch [69/300], Step [100/782], Loss: 1.3931\n",
            "Epoch [69/300], Step [200/782], Loss: 1.4435\n",
            "Epoch [69/300], Step [300/782], Loss: 1.3424\n",
            "Epoch [69/300], Step [400/782], Loss: 1.4123\n",
            "Epoch [69/300], Step [500/782], Loss: 1.3149\n",
            "Epoch [69/300], Step [600/782], Loss: 1.4593\n",
            "Epoch [69/300], Step [700/782], Loss: 1.4024\n",
            "Epoch [70/300], Step [100/782], Loss: 1.5259\n",
            "Epoch [70/300], Step [200/782], Loss: 1.3217\n",
            "Epoch [70/300], Step [300/782], Loss: 1.2191\n",
            "Epoch [70/300], Step [400/782], Loss: 1.6786\n",
            "Epoch [70/300], Step [500/782], Loss: 1.4815\n",
            "Epoch [70/300], Step [600/782], Loss: 1.2672\n",
            "Epoch [70/300], Step [700/782], Loss: 1.3854\n",
            "Epoch [71/300], Step [100/782], Loss: 1.3558\n",
            "Epoch [71/300], Step [200/782], Loss: 1.3548\n",
            "Epoch [71/300], Step [300/782], Loss: 1.3430\n",
            "Epoch [71/300], Step [400/782], Loss: 1.6058\n",
            "Epoch [71/300], Step [500/782], Loss: 1.3099\n",
            "Epoch [71/300], Step [600/782], Loss: 1.2163\n",
            "Epoch [71/300], Step [700/782], Loss: 1.3333\n",
            "Epoch [72/300], Step [100/782], Loss: 1.4503\n",
            "Epoch [72/300], Step [200/782], Loss: 1.3603\n",
            "Epoch [72/300], Step [300/782], Loss: 1.3538\n",
            "Epoch [72/300], Step [400/782], Loss: 1.1973\n",
            "Epoch [72/300], Step [500/782], Loss: 1.4464\n",
            "Epoch [72/300], Step [600/782], Loss: 1.1158\n",
            "Epoch [72/300], Step [700/782], Loss: 1.1954\n",
            "Epoch [73/300], Step [100/782], Loss: 1.5163\n",
            "Epoch [73/300], Step [200/782], Loss: 1.3264\n",
            "Epoch [73/300], Step [300/782], Loss: 1.3274\n",
            "Epoch [73/300], Step [400/782], Loss: 1.1785\n",
            "Epoch [73/300], Step [500/782], Loss: 1.3533\n",
            "Epoch [73/300], Step [600/782], Loss: 1.1342\n",
            "Epoch [73/300], Step [700/782], Loss: 1.2192\n",
            "Epoch [74/300], Step [100/782], Loss: 1.0604\n",
            "Epoch [74/300], Step [200/782], Loss: 1.6729\n",
            "Epoch [74/300], Step [300/782], Loss: 1.2527\n",
            "Epoch [74/300], Step [400/782], Loss: 1.2460\n",
            "Epoch [74/300], Step [500/782], Loss: 1.2860\n",
            "Epoch [74/300], Step [600/782], Loss: 1.3418\n",
            "Epoch [74/300], Step [700/782], Loss: 1.4122\n",
            "Epoch [75/300], Step [100/782], Loss: 1.4276\n",
            "Epoch [75/300], Step [200/782], Loss: 1.2817\n",
            "Epoch [75/300], Step [300/782], Loss: 1.2511\n",
            "Epoch [75/300], Step [400/782], Loss: 1.5073\n",
            "Epoch [75/300], Step [500/782], Loss: 1.1488\n",
            "Epoch [75/300], Step [600/782], Loss: 1.3074\n",
            "Epoch [75/300], Step [700/782], Loss: 1.1702\n",
            "Epoch [76/300], Step [100/782], Loss: 1.4490\n",
            "Epoch [76/300], Step [200/782], Loss: 1.3664\n",
            "Epoch [76/300], Step [300/782], Loss: 1.3123\n",
            "Epoch [76/300], Step [400/782], Loss: 1.2312\n",
            "Epoch [76/300], Step [500/782], Loss: 1.2479\n",
            "Epoch [76/300], Step [600/782], Loss: 1.2366\n",
            "Epoch [76/300], Step [700/782], Loss: 1.3614\n",
            "Epoch [77/300], Step [100/782], Loss: 1.1939\n",
            "Epoch [77/300], Step [200/782], Loss: 1.0000\n",
            "Epoch [77/300], Step [300/782], Loss: 1.3278\n",
            "Epoch [77/300], Step [400/782], Loss: 1.2280\n",
            "Epoch [77/300], Step [500/782], Loss: 1.3170\n",
            "Epoch [77/300], Step [600/782], Loss: 1.3380\n",
            "Epoch [77/300], Step [700/782], Loss: 1.1333\n",
            "Epoch [78/300], Step [100/782], Loss: 1.3971\n",
            "Epoch [78/300], Step [200/782], Loss: 1.2830\n",
            "Epoch [78/300], Step [300/782], Loss: 1.1999\n",
            "Epoch [78/300], Step [400/782], Loss: 1.2474\n",
            "Epoch [78/300], Step [500/782], Loss: 1.0819\n",
            "Epoch [78/300], Step [600/782], Loss: 1.2709\n",
            "Epoch [78/300], Step [700/782], Loss: 1.3791\n",
            "Epoch [79/300], Step [100/782], Loss: 1.1643\n",
            "Epoch [79/300], Step [200/782], Loss: 1.4727\n",
            "Epoch [79/300], Step [300/782], Loss: 1.2853\n",
            "Epoch [79/300], Step [400/782], Loss: 1.2092\n",
            "Epoch [79/300], Step [500/782], Loss: 1.3155\n",
            "Epoch [79/300], Step [600/782], Loss: 1.1530\n",
            "Epoch [79/300], Step [700/782], Loss: 1.3873\n",
            "Epoch [80/300], Step [100/782], Loss: 1.2130\n",
            "Epoch [80/300], Step [200/782], Loss: 1.3177\n",
            "Epoch [80/300], Step [300/782], Loss: 1.0462\n",
            "Epoch [80/300], Step [400/782], Loss: 1.3095\n",
            "Epoch [80/300], Step [500/782], Loss: 1.4056\n",
            "Epoch [80/300], Step [600/782], Loss: 1.4944\n",
            "Epoch [80/300], Step [700/782], Loss: 1.4372\n",
            "Epoch [81/300], Step [100/782], Loss: 1.1757\n",
            "Epoch [81/300], Step [200/782], Loss: 1.1133\n",
            "Epoch [81/300], Step [300/782], Loss: 1.3840\n",
            "Epoch [81/300], Step [400/782], Loss: 1.1432\n",
            "Epoch [81/300], Step [500/782], Loss: 1.3029\n",
            "Epoch [81/300], Step [600/782], Loss: 1.5260\n",
            "Epoch [81/300], Step [700/782], Loss: 1.4727\n",
            "Epoch [82/300], Step [100/782], Loss: 1.2855\n",
            "Epoch [82/300], Step [200/782], Loss: 1.0772\n",
            "Epoch [82/300], Step [300/782], Loss: 1.3240\n",
            "Epoch [82/300], Step [400/782], Loss: 1.2031\n",
            "Epoch [82/300], Step [500/782], Loss: 1.1411\n",
            "Epoch [82/300], Step [600/782], Loss: 1.4553\n",
            "Epoch [82/300], Step [700/782], Loss: 1.2318\n",
            "Epoch [83/300], Step [100/782], Loss: 1.5084\n",
            "Epoch [83/300], Step [200/782], Loss: 1.1249\n",
            "Epoch [83/300], Step [300/782], Loss: 1.2306\n",
            "Epoch [83/300], Step [400/782], Loss: 1.2473\n",
            "Epoch [83/300], Step [500/782], Loss: 1.3727\n",
            "Epoch [83/300], Step [600/782], Loss: 1.4950\n",
            "Epoch [83/300], Step [700/782], Loss: 1.3777\n",
            "Epoch [84/300], Step [100/782], Loss: 1.3256\n",
            "Epoch [84/300], Step [200/782], Loss: 1.2763\n",
            "Epoch [84/300], Step [300/782], Loss: 1.3176\n",
            "Epoch [84/300], Step [400/782], Loss: 1.3617\n",
            "Epoch [84/300], Step [500/782], Loss: 1.1203\n",
            "Epoch [84/300], Step [600/782], Loss: 1.2895\n",
            "Epoch [84/300], Step [700/782], Loss: 1.0642\n",
            "Epoch [85/300], Step [100/782], Loss: 1.1694\n",
            "Epoch [85/300], Step [200/782], Loss: 1.0872\n",
            "Epoch [85/300], Step [300/782], Loss: 1.3165\n",
            "Epoch [85/300], Step [400/782], Loss: 1.2363\n",
            "Epoch [85/300], Step [500/782], Loss: 1.4476\n",
            "Epoch [85/300], Step [600/782], Loss: 1.3341\n",
            "Epoch [85/300], Step [700/782], Loss: 1.1214\n",
            "Epoch [86/300], Step [100/782], Loss: 1.0637\n",
            "Epoch [86/300], Step [200/782], Loss: 1.0962\n",
            "Epoch [86/300], Step [300/782], Loss: 1.2151\n",
            "Epoch [86/300], Step [400/782], Loss: 1.2896\n",
            "Epoch [86/300], Step [500/782], Loss: 1.2438\n",
            "Epoch [86/300], Step [600/782], Loss: 1.1604\n",
            "Epoch [86/300], Step [700/782], Loss: 1.1002\n",
            "Epoch [87/300], Step [100/782], Loss: 1.2613\n",
            "Epoch [87/300], Step [200/782], Loss: 1.1506\n",
            "Epoch [87/300], Step [300/782], Loss: 1.4336\n",
            "Epoch [87/300], Step [400/782], Loss: 1.0961\n",
            "Epoch [87/300], Step [500/782], Loss: 1.4063\n",
            "Epoch [87/300], Step [600/782], Loss: 1.3779\n",
            "Epoch [87/300], Step [700/782], Loss: 1.4030\n",
            "Epoch [88/300], Step [100/782], Loss: 1.1661\n",
            "Epoch [88/300], Step [200/782], Loss: 1.2634\n",
            "Epoch [88/300], Step [300/782], Loss: 1.0536\n",
            "Epoch [88/300], Step [400/782], Loss: 1.1570\n",
            "Epoch [88/300], Step [500/782], Loss: 1.2670\n",
            "Epoch [88/300], Step [600/782], Loss: 1.5611\n",
            "Epoch [88/300], Step [700/782], Loss: 1.1348\n",
            "Epoch [89/300], Step [100/782], Loss: 1.1239\n",
            "Epoch [89/300], Step [200/782], Loss: 1.2176\n",
            "Epoch [89/300], Step [300/782], Loss: 1.1324\n",
            "Epoch [89/300], Step [400/782], Loss: 1.1603\n",
            "Epoch [89/300], Step [500/782], Loss: 1.2347\n",
            "Epoch [89/300], Step [600/782], Loss: 1.3519\n",
            "Epoch [89/300], Step [700/782], Loss: 1.3628\n",
            "Epoch [90/300], Step [100/782], Loss: 1.3672\n",
            "Epoch [90/300], Step [200/782], Loss: 1.1691\n",
            "Epoch [90/300], Step [300/782], Loss: 0.9852\n",
            "Epoch [90/300], Step [400/782], Loss: 1.0127\n",
            "Epoch [90/300], Step [500/782], Loss: 1.1450\n",
            "Epoch [90/300], Step [600/782], Loss: 1.2027\n",
            "Epoch [90/300], Step [700/782], Loss: 1.3060\n",
            "Epoch [91/300], Step [100/782], Loss: 1.4561\n",
            "Epoch [91/300], Step [200/782], Loss: 1.2309\n",
            "Epoch [91/300], Step [300/782], Loss: 1.2241\n",
            "Epoch [91/300], Step [400/782], Loss: 1.2346\n",
            "Epoch [91/300], Step [500/782], Loss: 1.0820\n",
            "Epoch [91/300], Step [600/782], Loss: 1.3010\n",
            "Epoch [91/300], Step [700/782], Loss: 1.0389\n",
            "Epoch [92/300], Step [100/782], Loss: 0.9716\n",
            "Epoch [92/300], Step [200/782], Loss: 1.1534\n",
            "Epoch [92/300], Step [300/782], Loss: 0.9811\n",
            "Epoch [92/300], Step [400/782], Loss: 1.1745\n",
            "Epoch [92/300], Step [500/782], Loss: 1.3085\n",
            "Epoch [92/300], Step [600/782], Loss: 1.3338\n",
            "Epoch [92/300], Step [700/782], Loss: 1.1707\n",
            "Epoch [93/300], Step [100/782], Loss: 1.5009\n",
            "Epoch [93/300], Step [200/782], Loss: 1.0086\n",
            "Epoch [93/300], Step [300/782], Loss: 1.1264\n",
            "Epoch [93/300], Step [400/782], Loss: 1.0885\n",
            "Epoch [93/300], Step [500/782], Loss: 1.1051\n",
            "Epoch [93/300], Step [600/782], Loss: 1.2251\n",
            "Epoch [93/300], Step [700/782], Loss: 1.1696\n",
            "Epoch [94/300], Step [100/782], Loss: 1.3651\n",
            "Epoch [94/300], Step [200/782], Loss: 1.3458\n",
            "Epoch [94/300], Step [300/782], Loss: 1.2527\n",
            "Epoch [94/300], Step [400/782], Loss: 1.1720\n",
            "Epoch [94/300], Step [500/782], Loss: 1.1691\n",
            "Epoch [94/300], Step [600/782], Loss: 1.1449\n",
            "Epoch [94/300], Step [700/782], Loss: 1.1634\n",
            "Epoch [95/300], Step [100/782], Loss: 1.0442\n",
            "Epoch [95/300], Step [200/782], Loss: 1.0026\n",
            "Epoch [95/300], Step [300/782], Loss: 1.1883\n",
            "Epoch [95/300], Step [400/782], Loss: 0.9249\n",
            "Epoch [95/300], Step [500/782], Loss: 1.1888\n",
            "Epoch [95/300], Step [600/782], Loss: 1.1631\n",
            "Epoch [95/300], Step [700/782], Loss: 1.0273\n",
            "Epoch [96/300], Step [100/782], Loss: 1.1650\n",
            "Epoch [96/300], Step [200/782], Loss: 1.1296\n",
            "Epoch [96/300], Step [300/782], Loss: 1.2227\n",
            "Epoch [96/300], Step [400/782], Loss: 1.2562\n",
            "Epoch [96/300], Step [500/782], Loss: 1.0590\n",
            "Epoch [96/300], Step [600/782], Loss: 1.1995\n",
            "Epoch [96/300], Step [700/782], Loss: 1.1447\n",
            "Epoch [97/300], Step [100/782], Loss: 1.0619\n",
            "Epoch [97/300], Step [200/782], Loss: 1.2726\n",
            "Epoch [97/300], Step [300/782], Loss: 0.8532\n",
            "Epoch [97/300], Step [400/782], Loss: 1.1726\n",
            "Epoch [97/300], Step [500/782], Loss: 1.2394\n",
            "Epoch [97/300], Step [600/782], Loss: 1.4643\n",
            "Epoch [97/300], Step [700/782], Loss: 1.2499\n",
            "Epoch [98/300], Step [100/782], Loss: 1.0266\n",
            "Epoch [98/300], Step [200/782], Loss: 1.2536\n",
            "Epoch [98/300], Step [300/782], Loss: 1.2207\n",
            "Epoch [98/300], Step [400/782], Loss: 1.3693\n",
            "Epoch [98/300], Step [500/782], Loss: 1.0477\n",
            "Epoch [98/300], Step [600/782], Loss: 0.9739\n",
            "Epoch [98/300], Step [700/782], Loss: 1.0466\n",
            "Epoch [99/300], Step [100/782], Loss: 1.1819\n",
            "Epoch [99/300], Step [200/782], Loss: 1.1504\n",
            "Epoch [99/300], Step [300/782], Loss: 1.3488\n",
            "Epoch [99/300], Step [400/782], Loss: 1.0350\n",
            "Epoch [99/300], Step [500/782], Loss: 1.0898\n",
            "Epoch [99/300], Step [600/782], Loss: 1.1909\n",
            "Epoch [99/300], Step [700/782], Loss: 1.0752\n",
            "Epoch [100/300], Step [100/782], Loss: 0.9474\n",
            "Epoch [100/300], Step [200/782], Loss: 1.0279\n",
            "Epoch [100/300], Step [300/782], Loss: 1.0662\n",
            "Epoch [100/300], Step [400/782], Loss: 1.2264\n",
            "Epoch [100/300], Step [500/782], Loss: 0.9494\n",
            "Epoch [100/300], Step [600/782], Loss: 1.2013\n",
            "Epoch [100/300], Step [700/782], Loss: 1.2941\n",
            "Epoch [101/300], Step [100/782], Loss: 1.3637\n",
            "Epoch [101/300], Step [200/782], Loss: 1.0134\n",
            "Epoch [101/300], Step [300/782], Loss: 1.0977\n",
            "Epoch [101/300], Step [400/782], Loss: 1.0026\n",
            "Epoch [101/300], Step [500/782], Loss: 1.3698\n",
            "Epoch [101/300], Step [600/782], Loss: 1.2109\n",
            "Epoch [101/300], Step [700/782], Loss: 1.0719\n",
            "Epoch [102/300], Step [100/782], Loss: 0.9561\n",
            "Epoch [102/300], Step [200/782], Loss: 1.2452\n",
            "Epoch [102/300], Step [300/782], Loss: 1.1644\n",
            "Epoch [102/300], Step [400/782], Loss: 1.0559\n",
            "Epoch [102/300], Step [500/782], Loss: 1.1449\n",
            "Epoch [102/300], Step [600/782], Loss: 0.9680\n",
            "Epoch [102/300], Step [700/782], Loss: 1.2299\n",
            "Epoch [103/300], Step [100/782], Loss: 0.9877\n",
            "Epoch [103/300], Step [200/782], Loss: 1.1696\n",
            "Epoch [103/300], Step [300/782], Loss: 1.0142\n",
            "Epoch [103/300], Step [400/782], Loss: 1.2486\n",
            "Epoch [103/300], Step [500/782], Loss: 0.9154\n",
            "Epoch [103/300], Step [600/782], Loss: 1.2671\n",
            "Epoch [103/300], Step [700/782], Loss: 1.0271\n",
            "Epoch [104/300], Step [100/782], Loss: 1.1416\n",
            "Epoch [104/300], Step [200/782], Loss: 1.2530\n",
            "Epoch [104/300], Step [300/782], Loss: 1.1337\n",
            "Epoch [104/300], Step [400/782], Loss: 1.0633\n",
            "Epoch [104/300], Step [500/782], Loss: 1.1821\n",
            "Epoch [104/300], Step [600/782], Loss: 1.2692\n",
            "Epoch [104/300], Step [700/782], Loss: 0.9495\n",
            "Epoch [105/300], Step [100/782], Loss: 1.0104\n",
            "Epoch [105/300], Step [200/782], Loss: 0.9857\n",
            "Epoch [105/300], Step [300/782], Loss: 1.2097\n",
            "Epoch [105/300], Step [400/782], Loss: 0.9816\n",
            "Epoch [105/300], Step [500/782], Loss: 1.1143\n",
            "Epoch [105/300], Step [600/782], Loss: 0.9306\n",
            "Epoch [105/300], Step [700/782], Loss: 0.9566\n",
            "Epoch [106/300], Step [100/782], Loss: 1.2562\n",
            "Epoch [106/300], Step [200/782], Loss: 1.1330\n",
            "Epoch [106/300], Step [300/782], Loss: 1.0666\n",
            "Epoch [106/300], Step [400/782], Loss: 0.9402\n",
            "Epoch [106/300], Step [500/782], Loss: 1.0295\n",
            "Epoch [106/300], Step [600/782], Loss: 1.4208\n",
            "Epoch [106/300], Step [700/782], Loss: 1.2172\n",
            "Epoch [107/300], Step [100/782], Loss: 1.1388\n",
            "Epoch [107/300], Step [200/782], Loss: 1.1824\n",
            "Epoch [107/300], Step [300/782], Loss: 1.0303\n",
            "Epoch [107/300], Step [400/782], Loss: 0.9737\n",
            "Epoch [107/300], Step [500/782], Loss: 1.1797\n",
            "Epoch [107/300], Step [600/782], Loss: 1.1617\n",
            "Epoch [107/300], Step [700/782], Loss: 1.1744\n",
            "Epoch [108/300], Step [100/782], Loss: 0.9374\n",
            "Epoch [108/300], Step [200/782], Loss: 1.1079\n",
            "Epoch [108/300], Step [300/782], Loss: 1.1534\n",
            "Epoch [108/300], Step [400/782], Loss: 1.0311\n",
            "Epoch [108/300], Step [500/782], Loss: 1.0515\n",
            "Epoch [108/300], Step [600/782], Loss: 1.0351\n",
            "Epoch [108/300], Step [700/782], Loss: 0.9649\n",
            "Epoch [109/300], Step [100/782], Loss: 1.0740\n",
            "Epoch [109/300], Step [200/782], Loss: 1.0742\n",
            "Epoch [109/300], Step [300/782], Loss: 1.1207\n",
            "Epoch [109/300], Step [400/782], Loss: 1.0091\n",
            "Epoch [109/300], Step [500/782], Loss: 1.2578\n",
            "Epoch [109/300], Step [600/782], Loss: 1.0998\n",
            "Epoch [109/300], Step [700/782], Loss: 0.9465\n",
            "Epoch [110/300], Step [100/782], Loss: 1.0980\n",
            "Epoch [110/300], Step [200/782], Loss: 1.4963\n",
            "Epoch [110/300], Step [300/782], Loss: 1.1822\n",
            "Epoch [110/300], Step [400/782], Loss: 1.0121\n",
            "Epoch [110/300], Step [500/782], Loss: 1.1477\n",
            "Epoch [110/300], Step [600/782], Loss: 0.8772\n",
            "Epoch [110/300], Step [700/782], Loss: 1.0601\n",
            "Epoch [111/300], Step [100/782], Loss: 0.9689\n",
            "Epoch [111/300], Step [200/782], Loss: 1.1441\n",
            "Epoch [111/300], Step [300/782], Loss: 0.9437\n",
            "Epoch [111/300], Step [400/782], Loss: 1.0783\n",
            "Epoch [111/300], Step [500/782], Loss: 0.9898\n",
            "Epoch [111/300], Step [600/782], Loss: 0.9986\n",
            "Epoch [111/300], Step [700/782], Loss: 1.0632\n",
            "Epoch [112/300], Step [100/782], Loss: 1.1245\n",
            "Epoch [112/300], Step [200/782], Loss: 1.3440\n",
            "Epoch [112/300], Step [300/782], Loss: 1.0174\n",
            "Epoch [112/300], Step [400/782], Loss: 0.9763\n",
            "Epoch [112/300], Step [500/782], Loss: 0.9043\n",
            "Epoch [112/300], Step [600/782], Loss: 0.8938\n",
            "Epoch [112/300], Step [700/782], Loss: 1.1521\n",
            "Epoch [113/300], Step [100/782], Loss: 0.9876\n",
            "Epoch [113/300], Step [200/782], Loss: 1.0895\n",
            "Epoch [113/300], Step [300/782], Loss: 1.1890\n",
            "Epoch [113/300], Step [400/782], Loss: 0.9864\n",
            "Epoch [113/300], Step [500/782], Loss: 0.9269\n",
            "Epoch [113/300], Step [600/782], Loss: 0.9843\n",
            "Epoch [113/300], Step [700/782], Loss: 1.1698\n",
            "Epoch [114/300], Step [100/782], Loss: 1.1244\n",
            "Epoch [114/300], Step [200/782], Loss: 1.0802\n",
            "Epoch [114/300], Step [300/782], Loss: 0.8072\n",
            "Epoch [114/300], Step [400/782], Loss: 1.0968\n",
            "Epoch [114/300], Step [500/782], Loss: 0.9273\n",
            "Epoch [114/300], Step [600/782], Loss: 1.1396\n",
            "Epoch [114/300], Step [700/782], Loss: 0.9133\n",
            "Epoch [115/300], Step [100/782], Loss: 0.9193\n",
            "Epoch [115/300], Step [200/782], Loss: 1.0774\n",
            "Epoch [115/300], Step [300/782], Loss: 0.9427\n",
            "Epoch [115/300], Step [400/782], Loss: 1.1049\n",
            "Epoch [115/300], Step [500/782], Loss: 0.9048\n",
            "Epoch [115/300], Step [600/782], Loss: 0.9854\n",
            "Epoch [115/300], Step [700/782], Loss: 0.7903\n",
            "Epoch [116/300], Step [100/782], Loss: 0.9425\n",
            "Epoch [116/300], Step [200/782], Loss: 1.3790\n",
            "Epoch [116/300], Step [300/782], Loss: 1.0020\n",
            "Epoch [116/300], Step [400/782], Loss: 1.1572\n",
            "Epoch [116/300], Step [500/782], Loss: 0.9668\n",
            "Epoch [116/300], Step [600/782], Loss: 0.9852\n",
            "Epoch [116/300], Step [700/782], Loss: 1.0600\n",
            "Epoch [117/300], Step [100/782], Loss: 0.9581\n",
            "Epoch [117/300], Step [200/782], Loss: 0.9883\n",
            "Epoch [117/300], Step [300/782], Loss: 1.2060\n",
            "Epoch [117/300], Step [400/782], Loss: 1.0356\n",
            "Epoch [117/300], Step [500/782], Loss: 1.1136\n",
            "Epoch [117/300], Step [600/782], Loss: 0.9570\n",
            "Epoch [117/300], Step [700/782], Loss: 0.9016\n",
            "Epoch [118/300], Step [100/782], Loss: 0.8329\n",
            "Epoch [118/300], Step [200/782], Loss: 0.9305\n",
            "Epoch [118/300], Step [300/782], Loss: 0.9742\n",
            "Epoch [118/300], Step [400/782], Loss: 1.3806\n",
            "Epoch [118/300], Step [500/782], Loss: 0.8746\n",
            "Epoch [118/300], Step [600/782], Loss: 1.0171\n",
            "Epoch [118/300], Step [700/782], Loss: 1.0648\n",
            "Epoch [119/300], Step [100/782], Loss: 0.8600\n",
            "Epoch [119/300], Step [200/782], Loss: 0.8008\n",
            "Epoch [119/300], Step [300/782], Loss: 0.9425\n",
            "Epoch [119/300], Step [400/782], Loss: 1.0529\n",
            "Epoch [119/300], Step [500/782], Loss: 0.8973\n",
            "Epoch [119/300], Step [600/782], Loss: 0.8592\n",
            "Epoch [119/300], Step [700/782], Loss: 0.9075\n",
            "Epoch [120/300], Step [100/782], Loss: 1.0907\n",
            "Epoch [120/300], Step [200/782], Loss: 0.8453\n",
            "Epoch [120/300], Step [300/782], Loss: 0.9985\n",
            "Epoch [120/300], Step [400/782], Loss: 0.8371\n",
            "Epoch [120/300], Step [500/782], Loss: 0.9764\n",
            "Epoch [120/300], Step [600/782], Loss: 0.9098\n",
            "Epoch [120/300], Step [700/782], Loss: 1.1068\n",
            "Epoch [121/300], Step [100/782], Loss: 0.8978\n",
            "Epoch [121/300], Step [200/782], Loss: 1.1476\n",
            "Epoch [121/300], Step [300/782], Loss: 1.0420\n",
            "Epoch [121/300], Step [400/782], Loss: 0.8874\n",
            "Epoch [121/300], Step [500/782], Loss: 1.0296\n",
            "Epoch [121/300], Step [600/782], Loss: 1.1166\n",
            "Epoch [121/300], Step [700/782], Loss: 1.1792\n",
            "Epoch [122/300], Step [100/782], Loss: 0.8896\n",
            "Epoch [122/300], Step [200/782], Loss: 0.9967\n",
            "Epoch [122/300], Step [300/782], Loss: 0.8767\n",
            "Epoch [122/300], Step [400/782], Loss: 0.9960\n",
            "Epoch [122/300], Step [500/782], Loss: 1.1801\n",
            "Epoch [122/300], Step [600/782], Loss: 1.0473\n",
            "Epoch [122/300], Step [700/782], Loss: 0.8257\n",
            "Epoch [123/300], Step [100/782], Loss: 0.9338\n",
            "Epoch [123/300], Step [200/782], Loss: 0.8043\n",
            "Epoch [123/300], Step [300/782], Loss: 1.0629\n",
            "Epoch [123/300], Step [400/782], Loss: 1.0379\n",
            "Epoch [123/300], Step [500/782], Loss: 0.9500\n",
            "Epoch [123/300], Step [600/782], Loss: 0.7203\n",
            "Epoch [123/300], Step [700/782], Loss: 0.9556\n",
            "Epoch [124/300], Step [100/782], Loss: 1.0994\n",
            "Epoch [124/300], Step [200/782], Loss: 0.8835\n",
            "Epoch [124/300], Step [300/782], Loss: 0.9310\n",
            "Epoch [124/300], Step [400/782], Loss: 0.8000\n",
            "Epoch [124/300], Step [500/782], Loss: 1.0354\n",
            "Epoch [124/300], Step [600/782], Loss: 1.0569\n",
            "Epoch [124/300], Step [700/782], Loss: 1.0005\n",
            "Epoch [125/300], Step [100/782], Loss: 0.9577\n",
            "Epoch [125/300], Step [200/782], Loss: 0.9377\n",
            "Epoch [125/300], Step [300/782], Loss: 1.0505\n",
            "Epoch [125/300], Step [400/782], Loss: 1.1839\n",
            "Epoch [125/300], Step [500/782], Loss: 0.9466\n",
            "Epoch [125/300], Step [600/782], Loss: 0.9332\n",
            "Epoch [125/300], Step [700/782], Loss: 0.9155\n",
            "Epoch [126/300], Step [100/782], Loss: 1.1423\n",
            "Epoch [126/300], Step [200/782], Loss: 0.7408\n",
            "Epoch [126/300], Step [300/782], Loss: 0.9225\n",
            "Epoch [126/300], Step [400/782], Loss: 0.7535\n",
            "Epoch [126/300], Step [500/782], Loss: 1.0002\n",
            "Epoch [126/300], Step [600/782], Loss: 0.8253\n",
            "Epoch [126/300], Step [700/782], Loss: 0.8554\n",
            "Epoch [127/300], Step [100/782], Loss: 1.0542\n",
            "Epoch [127/300], Step [200/782], Loss: 0.7600\n",
            "Epoch [127/300], Step [300/782], Loss: 0.9042\n",
            "Epoch [127/300], Step [400/782], Loss: 0.8818\n",
            "Epoch [127/300], Step [500/782], Loss: 1.0813\n",
            "Epoch [127/300], Step [600/782], Loss: 1.1305\n",
            "Epoch [127/300], Step [700/782], Loss: 0.7900\n",
            "Epoch [128/300], Step [100/782], Loss: 0.8374\n",
            "Epoch [128/300], Step [200/782], Loss: 0.9721\n",
            "Epoch [128/300], Step [300/782], Loss: 0.9061\n",
            "Epoch [128/300], Step [400/782], Loss: 1.0241\n",
            "Epoch [128/300], Step [500/782], Loss: 0.8619\n",
            "Epoch [128/300], Step [600/782], Loss: 0.7816\n",
            "Epoch [128/300], Step [700/782], Loss: 0.9177\n",
            "Epoch [129/300], Step [100/782], Loss: 0.9286\n",
            "Epoch [129/300], Step [200/782], Loss: 0.9181\n",
            "Epoch [129/300], Step [300/782], Loss: 0.9936\n",
            "Epoch [129/300], Step [400/782], Loss: 0.9088\n",
            "Epoch [129/300], Step [500/782], Loss: 1.0071\n",
            "Epoch [129/300], Step [600/782], Loss: 0.8480\n",
            "Epoch [129/300], Step [700/782], Loss: 1.0663\n",
            "Epoch [130/300], Step [100/782], Loss: 1.0203\n",
            "Epoch [130/300], Step [200/782], Loss: 0.9449\n",
            "Epoch [130/300], Step [300/782], Loss: 0.7220\n",
            "Epoch [130/300], Step [400/782], Loss: 0.8320\n",
            "Epoch [130/300], Step [500/782], Loss: 0.9167\n",
            "Epoch [130/300], Step [600/782], Loss: 0.8724\n",
            "Epoch [130/300], Step [700/782], Loss: 0.9058\n",
            "Epoch [131/300], Step [100/782], Loss: 0.8367\n",
            "Epoch [131/300], Step [200/782], Loss: 0.8315\n",
            "Epoch [131/300], Step [300/782], Loss: 0.7990\n",
            "Epoch [131/300], Step [400/782], Loss: 0.8214\n",
            "Epoch [131/300], Step [500/782], Loss: 1.0935\n",
            "Epoch [131/300], Step [600/782], Loss: 0.7714\n",
            "Epoch [131/300], Step [700/782], Loss: 0.9321\n",
            "Epoch [132/300], Step [100/782], Loss: 0.9681\n",
            "Epoch [132/300], Step [200/782], Loss: 0.8959\n",
            "Epoch [132/300], Step [300/782], Loss: 1.0422\n",
            "Epoch [132/300], Step [400/782], Loss: 1.0236\n",
            "Epoch [132/300], Step [500/782], Loss: 1.1644\n",
            "Epoch [132/300], Step [600/782], Loss: 0.8694\n",
            "Epoch [132/300], Step [700/782], Loss: 0.7585\n",
            "Epoch [133/300], Step [100/782], Loss: 0.7846\n",
            "Epoch [133/300], Step [200/782], Loss: 1.0891\n",
            "Epoch [133/300], Step [300/782], Loss: 1.0615\n",
            "Epoch [133/300], Step [400/782], Loss: 1.0053\n",
            "Epoch [133/300], Step [500/782], Loss: 0.7132\n",
            "Epoch [133/300], Step [600/782], Loss: 1.0953\n",
            "Epoch [133/300], Step [700/782], Loss: 0.9459\n",
            "Epoch [134/300], Step [100/782], Loss: 0.8939\n",
            "Epoch [134/300], Step [200/782], Loss: 0.8154\n",
            "Epoch [134/300], Step [300/782], Loss: 0.8158\n",
            "Epoch [134/300], Step [400/782], Loss: 0.7824\n",
            "Epoch [134/300], Step [500/782], Loss: 0.8343\n",
            "Epoch [134/300], Step [600/782], Loss: 0.7803\n",
            "Epoch [134/300], Step [700/782], Loss: 0.9834\n",
            "Epoch [135/300], Step [100/782], Loss: 1.1224\n",
            "Epoch [135/300], Step [200/782], Loss: 0.8291\n",
            "Epoch [135/300], Step [300/782], Loss: 0.8811\n",
            "Epoch [135/300], Step [400/782], Loss: 0.8288\n",
            "Epoch [135/300], Step [500/782], Loss: 0.8776\n",
            "Epoch [135/300], Step [600/782], Loss: 0.9194\n",
            "Epoch [135/300], Step [700/782], Loss: 0.9015\n",
            "Epoch [136/300], Step [100/782], Loss: 0.8079\n",
            "Epoch [136/300], Step [200/782], Loss: 0.9031\n",
            "Epoch [136/300], Step [300/782], Loss: 1.0707\n",
            "Epoch [136/300], Step [400/782], Loss: 0.8064\n",
            "Epoch [136/300], Step [500/782], Loss: 1.1395\n",
            "Epoch [136/300], Step [600/782], Loss: 1.0376\n",
            "Epoch [136/300], Step [700/782], Loss: 0.9219\n",
            "Epoch [137/300], Step [100/782], Loss: 0.8539\n",
            "Epoch [137/300], Step [200/782], Loss: 0.8010\n",
            "Epoch [137/300], Step [300/782], Loss: 0.8348\n",
            "Epoch [137/300], Step [400/782], Loss: 0.6746\n",
            "Epoch [137/300], Step [500/782], Loss: 1.0005\n",
            "Epoch [137/300], Step [600/782], Loss: 0.8522\n",
            "Epoch [137/300], Step [700/782], Loss: 0.9909\n",
            "Epoch [138/300], Step [100/782], Loss: 0.7847\n",
            "Epoch [138/300], Step [200/782], Loss: 0.9034\n",
            "Epoch [138/300], Step [300/782], Loss: 1.0078\n",
            "Epoch [138/300], Step [400/782], Loss: 0.8258\n",
            "Epoch [138/300], Step [500/782], Loss: 0.7174\n",
            "Epoch [138/300], Step [600/782], Loss: 0.7565\n",
            "Epoch [138/300], Step [700/782], Loss: 0.8392\n",
            "Epoch [139/300], Step [100/782], Loss: 0.7977\n",
            "Epoch [139/300], Step [200/782], Loss: 1.0143\n",
            "Epoch [139/300], Step [300/782], Loss: 0.7434\n",
            "Epoch [139/300], Step [400/782], Loss: 0.7524\n",
            "Epoch [139/300], Step [500/782], Loss: 0.7271\n",
            "Epoch [139/300], Step [600/782], Loss: 0.8784\n",
            "Epoch [139/300], Step [700/782], Loss: 0.8659\n",
            "Epoch [140/300], Step [100/782], Loss: 0.8162\n",
            "Epoch [140/300], Step [200/782], Loss: 0.7507\n",
            "Epoch [140/300], Step [300/782], Loss: 0.9106\n",
            "Epoch [140/300], Step [400/782], Loss: 0.8155\n",
            "Epoch [140/300], Step [500/782], Loss: 0.8482\n",
            "Epoch [140/300], Step [600/782], Loss: 0.7714\n",
            "Epoch [140/300], Step [700/782], Loss: 0.8142\n",
            "Epoch [141/300], Step [100/782], Loss: 0.6752\n",
            "Epoch [141/300], Step [200/782], Loss: 0.9555\n",
            "Epoch [141/300], Step [300/782], Loss: 0.7180\n",
            "Epoch [141/300], Step [400/782], Loss: 0.8067\n",
            "Epoch [141/300], Step [500/782], Loss: 0.9079\n",
            "Epoch [141/300], Step [600/782], Loss: 0.8279\n",
            "Epoch [141/300], Step [700/782], Loss: 0.8248\n",
            "Epoch [142/300], Step [100/782], Loss: 0.7956\n",
            "Epoch [142/300], Step [200/782], Loss: 0.9534\n",
            "Epoch [142/300], Step [300/782], Loss: 0.8435\n",
            "Epoch [142/300], Step [400/782], Loss: 0.8779\n",
            "Epoch [142/300], Step [500/782], Loss: 0.8230\n",
            "Epoch [142/300], Step [600/782], Loss: 0.8404\n",
            "Epoch [142/300], Step [700/782], Loss: 0.8967\n",
            "Epoch [143/300], Step [100/782], Loss: 0.7308\n",
            "Epoch [143/300], Step [200/782], Loss: 0.6754\n",
            "Epoch [143/300], Step [300/782], Loss: 0.7875\n",
            "Epoch [143/300], Step [400/782], Loss: 0.7502\n",
            "Epoch [143/300], Step [500/782], Loss: 0.7988\n",
            "Epoch [143/300], Step [600/782], Loss: 0.9120\n",
            "Epoch [143/300], Step [700/782], Loss: 0.8099\n",
            "Epoch [144/300], Step [100/782], Loss: 0.8807\n",
            "Epoch [144/300], Step [200/782], Loss: 0.7977\n",
            "Epoch [144/300], Step [300/782], Loss: 1.0118\n",
            "Epoch [144/300], Step [400/782], Loss: 0.6838\n",
            "Epoch [144/300], Step [500/782], Loss: 0.7105\n",
            "Epoch [144/300], Step [600/782], Loss: 0.8614\n",
            "Epoch [144/300], Step [700/782], Loss: 0.9011\n",
            "Epoch [145/300], Step [100/782], Loss: 0.7107\n",
            "Epoch [145/300], Step [200/782], Loss: 0.7679\n",
            "Epoch [145/300], Step [300/782], Loss: 0.6694\n",
            "Epoch [145/300], Step [400/782], Loss: 0.7831\n",
            "Epoch [145/300], Step [500/782], Loss: 0.6045\n",
            "Epoch [145/300], Step [600/782], Loss: 0.8383\n",
            "Epoch [145/300], Step [700/782], Loss: 0.8594\n",
            "Epoch [146/300], Step [100/782], Loss: 0.8374\n",
            "Epoch [146/300], Step [200/782], Loss: 1.0028\n",
            "Epoch [146/300], Step [300/782], Loss: 0.8215\n",
            "Epoch [146/300], Step [400/782], Loss: 0.6533\n",
            "Epoch [146/300], Step [500/782], Loss: 0.8881\n",
            "Epoch [146/300], Step [600/782], Loss: 0.7042\n",
            "Epoch [146/300], Step [700/782], Loss: 0.9314\n",
            "Epoch [147/300], Step [100/782], Loss: 0.7297\n",
            "Epoch [147/300], Step [200/782], Loss: 0.6664\n",
            "Epoch [147/300], Step [300/782], Loss: 0.8781\n",
            "Epoch [147/300], Step [400/782], Loss: 0.6549\n",
            "Epoch [147/300], Step [500/782], Loss: 0.8468\n",
            "Epoch [147/300], Step [600/782], Loss: 0.7670\n",
            "Epoch [147/300], Step [700/782], Loss: 0.8198\n",
            "Epoch [148/300], Step [100/782], Loss: 0.7889\n",
            "Epoch [148/300], Step [200/782], Loss: 0.8083\n",
            "Epoch [148/300], Step [300/782], Loss: 0.6846\n",
            "Epoch [148/300], Step [400/782], Loss: 0.9261\n",
            "Epoch [148/300], Step [500/782], Loss: 1.0197\n",
            "Epoch [148/300], Step [600/782], Loss: 0.8031\n",
            "Epoch [148/300], Step [700/782], Loss: 0.7054\n",
            "Epoch [149/300], Step [100/782], Loss: 0.6864\n",
            "Epoch [149/300], Step [200/782], Loss: 0.8523\n",
            "Epoch [149/300], Step [300/782], Loss: 0.7636\n",
            "Epoch [149/300], Step [400/782], Loss: 0.9228\n",
            "Epoch [149/300], Step [500/782], Loss: 0.8162\n",
            "Epoch [149/300], Step [600/782], Loss: 0.7714\n",
            "Epoch [149/300], Step [700/782], Loss: 0.7427\n",
            "Epoch [150/300], Step [100/782], Loss: 0.7456\n",
            "Epoch [150/300], Step [200/782], Loss: 0.6763\n",
            "Epoch [150/300], Step [300/782], Loss: 0.8041\n",
            "Epoch [150/300], Step [400/782], Loss: 0.9123\n",
            "Epoch [150/300], Step [500/782], Loss: 0.7292\n",
            "Epoch [150/300], Step [600/782], Loss: 0.8652\n",
            "Epoch [150/300], Step [700/782], Loss: 0.6927\n",
            "Epoch [151/300], Step [100/782], Loss: 0.7651\n",
            "Epoch [151/300], Step [200/782], Loss: 0.7331\n",
            "Epoch [151/300], Step [300/782], Loss: 0.7159\n",
            "Epoch [151/300], Step [400/782], Loss: 0.7249\n",
            "Epoch [151/300], Step [500/782], Loss: 0.7879\n",
            "Epoch [151/300], Step [600/782], Loss: 0.6143\n",
            "Epoch [151/300], Step [700/782], Loss: 0.8413\n",
            "Epoch [152/300], Step [100/782], Loss: 0.7390\n",
            "Epoch [152/300], Step [200/782], Loss: 0.7226\n",
            "Epoch [152/300], Step [300/782], Loss: 0.7769\n",
            "Epoch [152/300], Step [400/782], Loss: 0.6079\n",
            "Epoch [152/300], Step [500/782], Loss: 0.7273\n",
            "Epoch [152/300], Step [600/782], Loss: 0.7819\n",
            "Epoch [152/300], Step [700/782], Loss: 0.7649\n",
            "Epoch [153/300], Step [100/782], Loss: 0.7125\n",
            "Epoch [153/300], Step [200/782], Loss: 0.6441\n",
            "Epoch [153/300], Step [300/782], Loss: 0.8393\n",
            "Epoch [153/300], Step [400/782], Loss: 0.7748\n",
            "Epoch [153/300], Step [500/782], Loss: 0.7860\n",
            "Epoch [153/300], Step [600/782], Loss: 0.7162\n",
            "Epoch [153/300], Step [700/782], Loss: 0.8741\n",
            "Epoch [154/300], Step [100/782], Loss: 0.7836\n",
            "Epoch [154/300], Step [200/782], Loss: 0.6937\n",
            "Epoch [154/300], Step [300/782], Loss: 0.7376\n",
            "Epoch [154/300], Step [400/782], Loss: 0.8275\n",
            "Epoch [154/300], Step [500/782], Loss: 0.8141\n",
            "Epoch [154/300], Step [600/782], Loss: 0.6998\n",
            "Epoch [154/300], Step [700/782], Loss: 0.7367\n",
            "Epoch [155/300], Step [100/782], Loss: 0.6776\n",
            "Epoch [155/300], Step [200/782], Loss: 0.8024\n",
            "Epoch [155/300], Step [300/782], Loss: 0.7282\n",
            "Epoch [155/300], Step [400/782], Loss: 0.6608\n",
            "Epoch [155/300], Step [500/782], Loss: 0.7574\n",
            "Epoch [155/300], Step [600/782], Loss: 0.8127\n",
            "Epoch [155/300], Step [700/782], Loss: 0.9129\n",
            "Epoch [156/300], Step [100/782], Loss: 0.5947\n",
            "Epoch [156/300], Step [200/782], Loss: 0.7510\n",
            "Epoch [156/300], Step [300/782], Loss: 0.8053\n",
            "Epoch [156/300], Step [400/782], Loss: 0.6058\n",
            "Epoch [156/300], Step [500/782], Loss: 0.6394\n",
            "Epoch [156/300], Step [600/782], Loss: 0.5794\n",
            "Epoch [156/300], Step [700/782], Loss: 0.5346\n",
            "Epoch [157/300], Step [100/782], Loss: 0.7147\n",
            "Epoch [157/300], Step [200/782], Loss: 0.8151\n",
            "Epoch [157/300], Step [300/782], Loss: 0.6513\n",
            "Epoch [157/300], Step [400/782], Loss: 0.8834\n",
            "Epoch [157/300], Step [500/782], Loss: 0.6457\n",
            "Epoch [157/300], Step [600/782], Loss: 0.7944\n",
            "Epoch [157/300], Step [700/782], Loss: 0.7342\n",
            "Epoch [158/300], Step [100/782], Loss: 0.6196\n",
            "Epoch [158/300], Step [200/782], Loss: 0.6865\n",
            "Epoch [158/300], Step [300/782], Loss: 0.8907\n",
            "Epoch [158/300], Step [400/782], Loss: 0.6560\n",
            "Epoch [158/300], Step [500/782], Loss: 0.6403\n",
            "Epoch [158/300], Step [600/782], Loss: 0.7155\n",
            "Epoch [158/300], Step [700/782], Loss: 0.7821\n",
            "Epoch [159/300], Step [100/782], Loss: 0.6046\n",
            "Epoch [159/300], Step [200/782], Loss: 0.7397\n",
            "Epoch [159/300], Step [300/782], Loss: 0.5873\n",
            "Epoch [159/300], Step [400/782], Loss: 0.6092\n",
            "Epoch [159/300], Step [500/782], Loss: 0.7718\n",
            "Epoch [159/300], Step [600/782], Loss: 0.6813\n",
            "Epoch [159/300], Step [700/782], Loss: 0.8065\n",
            "Epoch [160/300], Step [100/782], Loss: 0.8888\n",
            "Epoch [160/300], Step [200/782], Loss: 0.8297\n",
            "Epoch [160/300], Step [300/782], Loss: 0.8647\n",
            "Epoch [160/300], Step [400/782], Loss: 0.6863\n",
            "Epoch [160/300], Step [500/782], Loss: 0.6822\n",
            "Epoch [160/300], Step [600/782], Loss: 0.6800\n",
            "Epoch [160/300], Step [700/782], Loss: 0.5836\n",
            "Epoch [161/300], Step [100/782], Loss: 0.6445\n",
            "Epoch [161/300], Step [200/782], Loss: 0.5350\n",
            "Epoch [161/300], Step [300/782], Loss: 0.7516\n",
            "Epoch [161/300], Step [400/782], Loss: 0.8085\n",
            "Epoch [161/300], Step [500/782], Loss: 0.7571\n",
            "Epoch [161/300], Step [600/782], Loss: 0.6854\n",
            "Epoch [161/300], Step [700/782], Loss: 0.7788\n",
            "Epoch [162/300], Step [100/782], Loss: 0.7427\n",
            "Epoch [162/300], Step [200/782], Loss: 0.6572\n",
            "Epoch [162/300], Step [300/782], Loss: 0.7377\n",
            "Epoch [162/300], Step [400/782], Loss: 0.6635\n",
            "Epoch [162/300], Step [500/782], Loss: 0.6087\n",
            "Epoch [162/300], Step [600/782], Loss: 0.7512\n",
            "Epoch [162/300], Step [700/782], Loss: 0.6450\n",
            "Epoch [163/300], Step [100/782], Loss: 0.5315\n",
            "Epoch [163/300], Step [200/782], Loss: 0.7258\n",
            "Epoch [163/300], Step [300/782], Loss: 0.7986\n",
            "Epoch [163/300], Step [400/782], Loss: 0.6793\n",
            "Epoch [163/300], Step [500/782], Loss: 0.7164\n",
            "Epoch [163/300], Step [600/782], Loss: 0.8836\n",
            "Epoch [163/300], Step [700/782], Loss: 0.6043\n",
            "Epoch [164/300], Step [100/782], Loss: 0.6526\n",
            "Epoch [164/300], Step [200/782], Loss: 0.6692\n",
            "Epoch [164/300], Step [300/782], Loss: 0.5833\n",
            "Epoch [164/300], Step [400/782], Loss: 0.6377\n",
            "Epoch [164/300], Step [500/782], Loss: 0.7188\n",
            "Epoch [164/300], Step [600/782], Loss: 0.7798\n",
            "Epoch [164/300], Step [700/782], Loss: 0.7609\n",
            "Epoch [165/300], Step [100/782], Loss: 0.5550\n",
            "Epoch [165/300], Step [200/782], Loss: 0.5639\n",
            "Epoch [165/300], Step [300/782], Loss: 0.6602\n",
            "Epoch [165/300], Step [400/782], Loss: 0.6828\n",
            "Epoch [165/300], Step [500/782], Loss: 0.5828\n",
            "Epoch [165/300], Step [600/782], Loss: 0.6936\n",
            "Epoch [165/300], Step [700/782], Loss: 0.5446\n",
            "Epoch [166/300], Step [100/782], Loss: 0.6438\n",
            "Epoch [166/300], Step [200/782], Loss: 0.5927\n",
            "Epoch [166/300], Step [300/782], Loss: 0.5977\n",
            "Epoch [166/300], Step [400/782], Loss: 0.6784\n",
            "Epoch [166/300], Step [500/782], Loss: 0.7443\n",
            "Epoch [166/300], Step [600/782], Loss: 0.6002\n",
            "Epoch [166/300], Step [700/782], Loss: 0.7052\n",
            "Epoch [167/300], Step [100/782], Loss: 0.6978\n",
            "Epoch [167/300], Step [200/782], Loss: 0.5747\n",
            "Epoch [167/300], Step [300/782], Loss: 0.8424\n",
            "Epoch [167/300], Step [400/782], Loss: 0.7216\n",
            "Epoch [167/300], Step [500/782], Loss: 0.7554\n",
            "Epoch [167/300], Step [600/782], Loss: 0.7045\n",
            "Epoch [167/300], Step [700/782], Loss: 0.7104\n",
            "Epoch [168/300], Step [100/782], Loss: 0.4820\n",
            "Epoch [168/300], Step [200/782], Loss: 0.6115\n",
            "Epoch [168/300], Step [300/782], Loss: 0.7651\n",
            "Epoch [168/300], Step [400/782], Loss: 0.5211\n",
            "Epoch [168/300], Step [500/782], Loss: 0.8391\n",
            "Epoch [168/300], Step [600/782], Loss: 0.8252\n",
            "Epoch [168/300], Step [700/782], Loss: 0.6414\n",
            "Epoch [169/300], Step [100/782], Loss: 0.6070\n",
            "Epoch [169/300], Step [200/782], Loss: 0.5969\n",
            "Epoch [169/300], Step [300/782], Loss: 0.6602\n",
            "Epoch [169/300], Step [400/782], Loss: 0.5539\n",
            "Epoch [169/300], Step [500/782], Loss: 0.6812\n",
            "Epoch [169/300], Step [600/782], Loss: 0.6380\n",
            "Epoch [169/300], Step [700/782], Loss: 0.6562\n",
            "Epoch [170/300], Step [100/782], Loss: 0.5019\n",
            "Epoch [170/300], Step [200/782], Loss: 0.7620\n",
            "Epoch [170/300], Step [300/782], Loss: 0.6413\n",
            "Epoch [170/300], Step [400/782], Loss: 0.5572\n",
            "Epoch [170/300], Step [500/782], Loss: 0.6362\n",
            "Epoch [170/300], Step [600/782], Loss: 0.7068\n",
            "Epoch [170/300], Step [700/782], Loss: 0.7364\n",
            "Epoch [171/300], Step [100/782], Loss: 0.5904\n",
            "Epoch [171/300], Step [200/782], Loss: 0.6371\n",
            "Epoch [171/300], Step [300/782], Loss: 0.5131\n",
            "Epoch [171/300], Step [400/782], Loss: 0.6800\n",
            "Epoch [171/300], Step [500/782], Loss: 0.6609\n",
            "Epoch [171/300], Step [600/782], Loss: 0.6303\n",
            "Epoch [171/300], Step [700/782], Loss: 0.5849\n",
            "Epoch [172/300], Step [100/782], Loss: 0.3926\n",
            "Epoch [172/300], Step [200/782], Loss: 0.5862\n",
            "Epoch [172/300], Step [300/782], Loss: 0.6213\n",
            "Epoch [172/300], Step [400/782], Loss: 0.7438\n",
            "Epoch [172/300], Step [500/782], Loss: 0.5914\n",
            "Epoch [172/300], Step [600/782], Loss: 0.5598\n",
            "Epoch [172/300], Step [700/782], Loss: 0.7295\n",
            "Epoch [173/300], Step [100/782], Loss: 0.4341\n",
            "Epoch [173/300], Step [200/782], Loss: 0.5645\n",
            "Epoch [173/300], Step [300/782], Loss: 0.5449\n",
            "Epoch [173/300], Step [400/782], Loss: 0.5908\n",
            "Epoch [173/300], Step [500/782], Loss: 0.6582\n",
            "Epoch [173/300], Step [600/782], Loss: 0.5876\n",
            "Epoch [173/300], Step [700/782], Loss: 0.5401\n",
            "Epoch [174/300], Step [100/782], Loss: 0.6840\n",
            "Epoch [174/300], Step [200/782], Loss: 0.4302\n",
            "Epoch [174/300], Step [300/782], Loss: 0.4970\n",
            "Epoch [174/300], Step [400/782], Loss: 0.5374\n",
            "Epoch [174/300], Step [500/782], Loss: 0.6075\n",
            "Epoch [174/300], Step [600/782], Loss: 0.3819\n",
            "Epoch [174/300], Step [700/782], Loss: 0.5693\n",
            "Epoch [175/300], Step [100/782], Loss: 0.5230\n",
            "Epoch [175/300], Step [200/782], Loss: 0.4454\n",
            "Epoch [175/300], Step [300/782], Loss: 0.6130\n",
            "Epoch [175/300], Step [400/782], Loss: 0.6235\n",
            "Epoch [175/300], Step [500/782], Loss: 0.7018\n",
            "Epoch [175/300], Step [600/782], Loss: 0.6066\n",
            "Epoch [175/300], Step [700/782], Loss: 0.6139\n",
            "Epoch [176/300], Step [100/782], Loss: 0.4988\n",
            "Epoch [176/300], Step [200/782], Loss: 0.7896\n",
            "Epoch [176/300], Step [300/782], Loss: 0.6827\n",
            "Epoch [176/300], Step [400/782], Loss: 0.5346\n",
            "Epoch [176/300], Step [500/782], Loss: 0.6319\n",
            "Epoch [176/300], Step [600/782], Loss: 0.6397\n",
            "Epoch [176/300], Step [700/782], Loss: 0.6407\n",
            "Epoch [177/300], Step [100/782], Loss: 0.5042\n",
            "Epoch [177/300], Step [200/782], Loss: 0.6018\n",
            "Epoch [177/300], Step [300/782], Loss: 0.5263\n",
            "Epoch [177/300], Step [400/782], Loss: 0.4572\n",
            "Epoch [177/300], Step [500/782], Loss: 0.6003\n",
            "Epoch [177/300], Step [600/782], Loss: 0.6425\n",
            "Epoch [177/300], Step [700/782], Loss: 0.5804\n",
            "Epoch [178/300], Step [100/782], Loss: 0.4251\n",
            "Epoch [178/300], Step [200/782], Loss: 0.5636\n",
            "Epoch [178/300], Step [300/782], Loss: 0.5572\n",
            "Epoch [178/300], Step [400/782], Loss: 0.6491\n",
            "Epoch [178/300], Step [500/782], Loss: 0.6245\n",
            "Epoch [178/300], Step [600/782], Loss: 0.5112\n",
            "Epoch [178/300], Step [700/782], Loss: 0.6263\n",
            "Epoch [179/300], Step [100/782], Loss: 0.5877\n",
            "Epoch [179/300], Step [200/782], Loss: 0.6278\n",
            "Epoch [179/300], Step [300/782], Loss: 0.4519\n",
            "Epoch [179/300], Step [400/782], Loss: 0.4750\n",
            "Epoch [179/300], Step [500/782], Loss: 0.6297\n",
            "Epoch [179/300], Step [600/782], Loss: 0.6724\n",
            "Epoch [179/300], Step [700/782], Loss: 0.4634\n",
            "Epoch [180/300], Step [100/782], Loss: 0.5780\n",
            "Epoch [180/300], Step [200/782], Loss: 0.6522\n",
            "Epoch [180/300], Step [300/782], Loss: 0.5458\n",
            "Epoch [180/300], Step [400/782], Loss: 0.6012\n",
            "Epoch [180/300], Step [500/782], Loss: 0.5063\n",
            "Epoch [180/300], Step [600/782], Loss: 0.7791\n",
            "Epoch [180/300], Step [700/782], Loss: 0.6790\n",
            "Epoch [181/300], Step [100/782], Loss: 0.4562\n",
            "Epoch [181/300], Step [200/782], Loss: 0.6342\n",
            "Epoch [181/300], Step [300/782], Loss: 0.5227\n",
            "Epoch [181/300], Step [400/782], Loss: 0.6216\n",
            "Epoch [181/300], Step [500/782], Loss: 0.7556\n",
            "Epoch [181/300], Step [600/782], Loss: 0.6479\n",
            "Epoch [181/300], Step [700/782], Loss: 0.6557\n",
            "Epoch [182/300], Step [100/782], Loss: 0.4476\n",
            "Epoch [182/300], Step [200/782], Loss: 0.6112\n",
            "Epoch [182/300], Step [300/782], Loss: 0.5107\n",
            "Epoch [182/300], Step [400/782], Loss: 0.4212\n",
            "Epoch [182/300], Step [500/782], Loss: 0.4436\n",
            "Epoch [182/300], Step [600/782], Loss: 0.6295\n",
            "Epoch [182/300], Step [700/782], Loss: 0.5740\n",
            "Epoch [183/300], Step [100/782], Loss: 0.6369\n",
            "Epoch [183/300], Step [200/782], Loss: 0.5891\n",
            "Epoch [183/300], Step [300/782], Loss: 0.4942\n",
            "Epoch [183/300], Step [400/782], Loss: 0.4548\n",
            "Epoch [183/300], Step [500/782], Loss: 0.5933\n",
            "Epoch [183/300], Step [600/782], Loss: 0.7515\n",
            "Epoch [183/300], Step [700/782], Loss: 0.6874\n",
            "Epoch [184/300], Step [100/782], Loss: 0.4877\n",
            "Epoch [184/300], Step [200/782], Loss: 0.4544\n",
            "Epoch [184/300], Step [300/782], Loss: 0.4580\n",
            "Epoch [184/300], Step [400/782], Loss: 0.5196\n",
            "Epoch [184/300], Step [500/782], Loss: 0.4037\n",
            "Epoch [184/300], Step [600/782], Loss: 0.5764\n",
            "Epoch [184/300], Step [700/782], Loss: 0.5683\n",
            "Epoch [185/300], Step [100/782], Loss: 0.5022\n",
            "Epoch [185/300], Step [200/782], Loss: 0.3823\n",
            "Epoch [185/300], Step [300/782], Loss: 0.4629\n",
            "Epoch [185/300], Step [400/782], Loss: 0.5061\n",
            "Epoch [185/300], Step [500/782], Loss: 0.4417\n",
            "Epoch [185/300], Step [600/782], Loss: 0.6546\n",
            "Epoch [185/300], Step [700/782], Loss: 0.5722\n",
            "Epoch [186/300], Step [100/782], Loss: 0.4703\n",
            "Epoch [186/300], Step [200/782], Loss: 0.4883\n",
            "Epoch [186/300], Step [300/782], Loss: 0.4198\n",
            "Epoch [186/300], Step [400/782], Loss: 0.5665\n",
            "Epoch [186/300], Step [500/782], Loss: 0.6192\n",
            "Epoch [186/300], Step [600/782], Loss: 0.3838\n",
            "Epoch [186/300], Step [700/782], Loss: 0.6719\n",
            "Epoch [187/300], Step [100/782], Loss: 0.5738\n",
            "Epoch [187/300], Step [200/782], Loss: 0.6610\n",
            "Epoch [187/300], Step [300/782], Loss: 0.5602\n",
            "Epoch [187/300], Step [400/782], Loss: 0.4699\n",
            "Epoch [187/300], Step [500/782], Loss: 0.4275\n",
            "Epoch [187/300], Step [600/782], Loss: 0.5323\n",
            "Epoch [187/300], Step [700/782], Loss: 0.4165\n",
            "Epoch [188/300], Step [100/782], Loss: 0.5549\n",
            "Epoch [188/300], Step [200/782], Loss: 0.5093\n",
            "Epoch [188/300], Step [300/782], Loss: 0.6117\n",
            "Epoch [188/300], Step [400/782], Loss: 0.5343\n",
            "Epoch [188/300], Step [500/782], Loss: 0.4768\n",
            "Epoch [188/300], Step [600/782], Loss: 0.3940\n",
            "Epoch [188/300], Step [700/782], Loss: 0.4451\n",
            "Epoch [189/300], Step [100/782], Loss: 0.4262\n",
            "Epoch [189/300], Step [200/782], Loss: 0.5316\n",
            "Epoch [189/300], Step [300/782], Loss: 0.4903\n",
            "Epoch [189/300], Step [400/782], Loss: 0.5810\n",
            "Epoch [189/300], Step [500/782], Loss: 0.4902\n",
            "Epoch [189/300], Step [600/782], Loss: 0.5497\n",
            "Epoch [189/300], Step [700/782], Loss: 0.5004\n",
            "Epoch [190/300], Step [100/782], Loss: 0.5425\n",
            "Epoch [190/300], Step [200/782], Loss: 0.5238\n",
            "Epoch [190/300], Step [300/782], Loss: 0.5814\n",
            "Epoch [190/300], Step [400/782], Loss: 0.4590\n",
            "Epoch [190/300], Step [500/782], Loss: 0.3900\n",
            "Epoch [190/300], Step [600/782], Loss: 0.5755\n",
            "Epoch [190/300], Step [700/782], Loss: 0.6780\n",
            "Epoch [191/300], Step [100/782], Loss: 0.5107\n",
            "Epoch [191/300], Step [200/782], Loss: 0.4958\n",
            "Epoch [191/300], Step [300/782], Loss: 0.5478\n",
            "Epoch [191/300], Step [400/782], Loss: 0.3324\n",
            "Epoch [191/300], Step [500/782], Loss: 0.5138\n",
            "Epoch [191/300], Step [600/782], Loss: 0.4601\n",
            "Epoch [191/300], Step [700/782], Loss: 0.4512\n",
            "Epoch [192/300], Step [100/782], Loss: 0.6034\n",
            "Epoch [192/300], Step [200/782], Loss: 0.4730\n",
            "Epoch [192/300], Step [300/782], Loss: 0.4550\n",
            "Epoch [192/300], Step [400/782], Loss: 0.3582\n",
            "Epoch [192/300], Step [500/782], Loss: 0.5834\n",
            "Epoch [192/300], Step [600/782], Loss: 0.5157\n",
            "Epoch [192/300], Step [700/782], Loss: 0.5978\n",
            "Epoch [193/300], Step [100/782], Loss: 0.4390\n",
            "Epoch [193/300], Step [200/782], Loss: 0.4660\n",
            "Epoch [193/300], Step [300/782], Loss: 0.3712\n",
            "Epoch [193/300], Step [400/782], Loss: 0.4563\n",
            "Epoch [193/300], Step [500/782], Loss: 0.4939\n",
            "Epoch [193/300], Step [600/782], Loss: 0.4307\n",
            "Epoch [193/300], Step [700/782], Loss: 0.4694\n",
            "Epoch [194/300], Step [100/782], Loss: 0.3058\n",
            "Epoch [194/300], Step [200/782], Loss: 0.3493\n",
            "Epoch [194/300], Step [300/782], Loss: 0.4648\n",
            "Epoch [194/300], Step [400/782], Loss: 0.4954\n",
            "Epoch [194/300], Step [500/782], Loss: 0.2903\n",
            "Epoch [194/300], Step [600/782], Loss: 0.4283\n",
            "Epoch [194/300], Step [700/782], Loss: 0.5221\n",
            "Epoch [195/300], Step [100/782], Loss: 0.4700\n",
            "Epoch [195/300], Step [200/782], Loss: 0.3562\n",
            "Epoch [195/300], Step [300/782], Loss: 0.4026\n",
            "Epoch [195/300], Step [400/782], Loss: 0.4463\n",
            "Epoch [195/300], Step [500/782], Loss: 0.4527\n",
            "Epoch [195/300], Step [600/782], Loss: 0.6209\n",
            "Epoch [195/300], Step [700/782], Loss: 0.5461\n",
            "Epoch [196/300], Step [100/782], Loss: 0.4783\n",
            "Epoch [196/300], Step [200/782], Loss: 0.4232\n",
            "Epoch [196/300], Step [300/782], Loss: 0.2881\n",
            "Epoch [196/300], Step [400/782], Loss: 0.3994\n",
            "Epoch [196/300], Step [500/782], Loss: 0.3565\n",
            "Epoch [196/300], Step [600/782], Loss: 0.3864\n",
            "Epoch [196/300], Step [700/782], Loss: 0.4179\n",
            "Epoch [197/300], Step [100/782], Loss: 0.3506\n",
            "Epoch [197/300], Step [200/782], Loss: 0.3903\n",
            "Epoch [197/300], Step [300/782], Loss: 0.3735\n",
            "Epoch [197/300], Step [400/782], Loss: 0.5000\n",
            "Epoch [197/300], Step [500/782], Loss: 0.4640\n",
            "Epoch [197/300], Step [600/782], Loss: 0.4594\n",
            "Epoch [197/300], Step [700/782], Loss: 0.4566\n",
            "Epoch [198/300], Step [100/782], Loss: 0.4417\n",
            "Epoch [198/300], Step [200/782], Loss: 0.4915\n",
            "Epoch [198/300], Step [300/782], Loss: 0.5003\n",
            "Epoch [198/300], Step [400/782], Loss: 0.5437\n",
            "Epoch [198/300], Step [500/782], Loss: 0.4763\n",
            "Epoch [198/300], Step [600/782], Loss: 0.4450\n",
            "Epoch [198/300], Step [700/782], Loss: 0.4102\n",
            "Epoch [199/300], Step [100/782], Loss: 0.5093\n",
            "Epoch [199/300], Step [200/782], Loss: 0.4058\n",
            "Epoch [199/300], Step [300/782], Loss: 0.4575\n",
            "Epoch [199/300], Step [400/782], Loss: 0.3051\n",
            "Epoch [199/300], Step [500/782], Loss: 0.4579\n",
            "Epoch [199/300], Step [600/782], Loss: 0.4019\n",
            "Epoch [199/300], Step [700/782], Loss: 0.4394\n",
            "Epoch [200/300], Step [100/782], Loss: 0.4327\n",
            "Epoch [200/300], Step [200/782], Loss: 0.3335\n",
            "Epoch [200/300], Step [300/782], Loss: 0.3842\n",
            "Epoch [200/300], Step [400/782], Loss: 0.4461\n",
            "Epoch [200/300], Step [500/782], Loss: 0.4306\n",
            "Epoch [200/300], Step [600/782], Loss: 0.4441\n",
            "Epoch [200/300], Step [700/782], Loss: 0.4986\n",
            "Epoch [201/300], Step [100/782], Loss: 0.4374\n",
            "Epoch [201/300], Step [200/782], Loss: 0.3870\n",
            "Epoch [201/300], Step [300/782], Loss: 0.3019\n",
            "Epoch [201/300], Step [400/782], Loss: 0.3758\n",
            "Epoch [201/300], Step [500/782], Loss: 0.3926\n",
            "Epoch [201/300], Step [600/782], Loss: 0.3389\n",
            "Epoch [201/300], Step [700/782], Loss: 0.3968\n",
            "Epoch [202/300], Step [100/782], Loss: 0.4269\n",
            "Epoch [202/300], Step [200/782], Loss: 0.2800\n",
            "Epoch [202/300], Step [300/782], Loss: 0.4421\n",
            "Epoch [202/300], Step [400/782], Loss: 0.4912\n",
            "Epoch [202/300], Step [500/782], Loss: 0.3278\n",
            "Epoch [202/300], Step [600/782], Loss: 0.3208\n",
            "Epoch [202/300], Step [700/782], Loss: 0.3158\n",
            "Epoch [203/300], Step [100/782], Loss: 0.3408\n",
            "Epoch [203/300], Step [200/782], Loss: 0.3696\n",
            "Epoch [203/300], Step [300/782], Loss: 0.3650\n",
            "Epoch [203/300], Step [400/782], Loss: 0.3397\n",
            "Epoch [203/300], Step [500/782], Loss: 0.3692\n",
            "Epoch [203/300], Step [600/782], Loss: 0.5423\n",
            "Epoch [203/300], Step [700/782], Loss: 0.2686\n",
            "Epoch [204/300], Step [100/782], Loss: 0.3548\n",
            "Epoch [204/300], Step [200/782], Loss: 0.2697\n",
            "Epoch [204/300], Step [300/782], Loss: 0.3227\n",
            "Epoch [204/300], Step [400/782], Loss: 0.4392\n",
            "Epoch [204/300], Step [500/782], Loss: 0.3442\n",
            "Epoch [204/300], Step [600/782], Loss: 0.5321\n",
            "Epoch [204/300], Step [700/782], Loss: 0.3524\n",
            "Epoch [205/300], Step [100/782], Loss: 0.2928\n",
            "Epoch [205/300], Step [200/782], Loss: 0.4697\n",
            "Epoch [205/300], Step [300/782], Loss: 0.2806\n",
            "Epoch [205/300], Step [400/782], Loss: 0.5419\n",
            "Epoch [205/300], Step [500/782], Loss: 0.3403\n",
            "Epoch [205/300], Step [600/782], Loss: 0.3054\n",
            "Epoch [205/300], Step [700/782], Loss: 0.3018\n",
            "Epoch [206/300], Step [100/782], Loss: 0.2808\n",
            "Epoch [206/300], Step [200/782], Loss: 0.3578\n",
            "Epoch [206/300], Step [300/782], Loss: 0.4049\n",
            "Epoch [206/300], Step [400/782], Loss: 0.3432\n",
            "Epoch [206/300], Step [500/782], Loss: 0.3559\n",
            "Epoch [206/300], Step [600/782], Loss: 0.3788\n",
            "Epoch [206/300], Step [700/782], Loss: 0.4684\n",
            "Epoch [207/300], Step [100/782], Loss: 0.3709\n",
            "Epoch [207/300], Step [200/782], Loss: 0.3086\n",
            "Epoch [207/300], Step [300/782], Loss: 0.2969\n",
            "Epoch [207/300], Step [400/782], Loss: 0.3974\n",
            "Epoch [207/300], Step [500/782], Loss: 0.5845\n",
            "Epoch [207/300], Step [600/782], Loss: 0.3371\n",
            "Epoch [207/300], Step [700/782], Loss: 0.3698\n",
            "Epoch [208/300], Step [100/782], Loss: 0.2710\n",
            "Epoch [208/300], Step [200/782], Loss: 0.2667\n",
            "Epoch [208/300], Step [300/782], Loss: 0.3040\n",
            "Epoch [208/300], Step [400/782], Loss: 0.3100\n",
            "Epoch [208/300], Step [500/782], Loss: 0.3556\n",
            "Epoch [208/300], Step [600/782], Loss: 0.4298\n",
            "Epoch [208/300], Step [700/782], Loss: 0.3213\n",
            "Epoch [209/300], Step [100/782], Loss: 0.4600\n",
            "Epoch [209/300], Step [200/782], Loss: 0.3089\n",
            "Epoch [209/300], Step [300/782], Loss: 0.4134\n",
            "Epoch [209/300], Step [400/782], Loss: 0.2825\n",
            "Epoch [209/300], Step [500/782], Loss: 0.3348\n",
            "Epoch [209/300], Step [600/782], Loss: 0.5306\n",
            "Epoch [209/300], Step [700/782], Loss: 0.3738\n",
            "Epoch [210/300], Step [100/782], Loss: 0.2455\n",
            "Epoch [210/300], Step [200/782], Loss: 0.2531\n",
            "Epoch [210/300], Step [300/782], Loss: 0.4616\n",
            "Epoch [210/300], Step [400/782], Loss: 0.2360\n",
            "Epoch [210/300], Step [500/782], Loss: 0.3478\n",
            "Epoch [210/300], Step [600/782], Loss: 0.3378\n",
            "Epoch [210/300], Step [700/782], Loss: 0.4709\n",
            "Epoch [211/300], Step [100/782], Loss: 0.5525\n",
            "Epoch [211/300], Step [200/782], Loss: 0.3125\n",
            "Epoch [211/300], Step [300/782], Loss: 0.4335\n",
            "Epoch [211/300], Step [400/782], Loss: 0.3350\n",
            "Epoch [211/300], Step [500/782], Loss: 0.4603\n",
            "Epoch [211/300], Step [600/782], Loss: 0.2864\n",
            "Epoch [211/300], Step [700/782], Loss: 0.3065\n",
            "Epoch [212/300], Step [100/782], Loss: 0.3977\n",
            "Epoch [212/300], Step [200/782], Loss: 0.2390\n",
            "Epoch [212/300], Step [300/782], Loss: 0.2404\n",
            "Epoch [212/300], Step [400/782], Loss: 0.2538\n",
            "Epoch [212/300], Step [500/782], Loss: 0.3446\n",
            "Epoch [212/300], Step [600/782], Loss: 0.3858\n",
            "Epoch [212/300], Step [700/782], Loss: 0.2753\n",
            "Epoch [213/300], Step [100/782], Loss: 0.4471\n",
            "Epoch [213/300], Step [200/782], Loss: 0.3619\n",
            "Epoch [213/300], Step [300/782], Loss: 0.3509\n",
            "Epoch [213/300], Step [400/782], Loss: 0.3428\n",
            "Epoch [213/300], Step [500/782], Loss: 0.4166\n",
            "Epoch [213/300], Step [600/782], Loss: 0.3767\n",
            "Epoch [213/300], Step [700/782], Loss: 0.3706\n",
            "Epoch [214/300], Step [100/782], Loss: 0.3659\n",
            "Epoch [214/300], Step [200/782], Loss: 0.2325\n",
            "Epoch [214/300], Step [300/782], Loss: 0.3398\n",
            "Epoch [214/300], Step [400/782], Loss: 0.3774\n",
            "Epoch [214/300], Step [500/782], Loss: 0.3567\n",
            "Epoch [214/300], Step [600/782], Loss: 0.3128\n",
            "Epoch [214/300], Step [700/782], Loss: 0.3988\n",
            "Epoch [215/300], Step [100/782], Loss: 0.2950\n",
            "Epoch [215/300], Step [200/782], Loss: 0.1883\n",
            "Epoch [215/300], Step [300/782], Loss: 0.2339\n",
            "Epoch [215/300], Step [400/782], Loss: 0.3812\n",
            "Epoch [215/300], Step [500/782], Loss: 0.3339\n",
            "Epoch [215/300], Step [600/782], Loss: 0.4055\n",
            "Epoch [215/300], Step [700/782], Loss: 0.2660\n",
            "Epoch [216/300], Step [100/782], Loss: 0.3242\n",
            "Epoch [216/300], Step [200/782], Loss: 0.3211\n",
            "Epoch [216/300], Step [300/782], Loss: 0.3482\n",
            "Epoch [216/300], Step [400/782], Loss: 0.4045\n",
            "Epoch [216/300], Step [500/782], Loss: 0.3075\n",
            "Epoch [216/300], Step [600/782], Loss: 0.3258\n",
            "Epoch [216/300], Step [700/782], Loss: 0.3353\n",
            "Epoch [217/300], Step [100/782], Loss: 0.2427\n",
            "Epoch [217/300], Step [200/782], Loss: 0.3683\n",
            "Epoch [217/300], Step [300/782], Loss: 0.2507\n",
            "Epoch [217/300], Step [400/782], Loss: 0.2922\n",
            "Epoch [217/300], Step [500/782], Loss: 0.2574\n",
            "Epoch [217/300], Step [600/782], Loss: 0.3136\n",
            "Epoch [217/300], Step [700/782], Loss: 0.2718\n",
            "Epoch [218/300], Step [100/782], Loss: 0.3735\n",
            "Epoch [218/300], Step [200/782], Loss: 0.4496\n",
            "Epoch [218/300], Step [300/782], Loss: 0.3709\n",
            "Epoch [218/300], Step [400/782], Loss: 0.4139\n",
            "Epoch [218/300], Step [500/782], Loss: 0.3380\n",
            "Epoch [218/300], Step [600/782], Loss: 0.4636\n",
            "Epoch [218/300], Step [700/782], Loss: 0.3626\n",
            "Epoch [219/300], Step [100/782], Loss: 0.3163\n",
            "Epoch [219/300], Step [200/782], Loss: 0.2306\n",
            "Epoch [219/300], Step [300/782], Loss: 0.2707\n",
            "Epoch [219/300], Step [400/782], Loss: 0.2334\n",
            "Epoch [219/300], Step [500/782], Loss: 0.2662\n",
            "Epoch [219/300], Step [600/782], Loss: 0.3538\n",
            "Epoch [219/300], Step [700/782], Loss: 0.3180\n",
            "Epoch [220/300], Step [100/782], Loss: 0.2237\n",
            "Epoch [220/300], Step [200/782], Loss: 0.1907\n",
            "Epoch [220/300], Step [300/782], Loss: 0.2487\n",
            "Epoch [220/300], Step [400/782], Loss: 0.1897\n",
            "Epoch [220/300], Step [500/782], Loss: 0.2240\n",
            "Epoch [220/300], Step [600/782], Loss: 0.2563\n",
            "Epoch [220/300], Step [700/782], Loss: 0.3079\n",
            "Epoch [221/300], Step [100/782], Loss: 0.3629\n",
            "Epoch [221/300], Step [200/782], Loss: 0.5636\n",
            "Epoch [221/300], Step [300/782], Loss: 0.2342\n",
            "Epoch [221/300], Step [400/782], Loss: 0.2689\n",
            "Epoch [221/300], Step [500/782], Loss: 0.2633\n",
            "Epoch [221/300], Step [600/782], Loss: 0.2758\n",
            "Epoch [221/300], Step [700/782], Loss: 0.3519\n",
            "Epoch [222/300], Step [100/782], Loss: 0.3140\n",
            "Epoch [222/300], Step [200/782], Loss: 0.2801\n",
            "Epoch [222/300], Step [300/782], Loss: 0.3931\n",
            "Epoch [222/300], Step [400/782], Loss: 0.2884\n",
            "Epoch [222/300], Step [500/782], Loss: 0.2509\n",
            "Epoch [222/300], Step [600/782], Loss: 0.3463\n",
            "Epoch [222/300], Step [700/782], Loss: 0.3313\n",
            "Epoch [223/300], Step [100/782], Loss: 0.3251\n",
            "Epoch [223/300], Step [200/782], Loss: 0.3221\n",
            "Epoch [223/300], Step [300/782], Loss: 0.4761\n",
            "Epoch [223/300], Step [400/782], Loss: 0.3203\n",
            "Epoch [223/300], Step [500/782], Loss: 0.2170\n",
            "Epoch [223/300], Step [600/782], Loss: 0.2087\n",
            "Epoch [223/300], Step [700/782], Loss: 0.2803\n",
            "Epoch [224/300], Step [100/782], Loss: 0.3846\n",
            "Epoch [224/300], Step [200/782], Loss: 0.4026\n",
            "Epoch [224/300], Step [300/782], Loss: 0.3340\n",
            "Epoch [224/300], Step [400/782], Loss: 0.2948\n",
            "Epoch [224/300], Step [500/782], Loss: 0.2632\n",
            "Epoch [224/300], Step [600/782], Loss: 0.2394\n",
            "Epoch [224/300], Step [700/782], Loss: 0.4202\n",
            "Epoch [225/300], Step [100/782], Loss: 0.3077\n",
            "Epoch [225/300], Step [200/782], Loss: 0.2186\n",
            "Epoch [225/300], Step [300/782], Loss: 0.2935\n",
            "Epoch [225/300], Step [400/782], Loss: 0.2273\n",
            "Epoch [225/300], Step [500/782], Loss: 0.2525\n",
            "Epoch [225/300], Step [600/782], Loss: 0.2742\n",
            "Epoch [225/300], Step [700/782], Loss: 0.2953\n",
            "Epoch [226/300], Step [100/782], Loss: 0.3627\n",
            "Epoch [226/300], Step [200/782], Loss: 0.3160\n",
            "Epoch [226/300], Step [300/782], Loss: 0.2907\n",
            "Epoch [226/300], Step [400/782], Loss: 0.3097\n",
            "Epoch [226/300], Step [500/782], Loss: 0.2644\n",
            "Epoch [226/300], Step [600/782], Loss: 0.3005\n",
            "Epoch [226/300], Step [700/782], Loss: 0.3246\n",
            "Epoch [227/300], Step [100/782], Loss: 0.2694\n",
            "Epoch [227/300], Step [200/782], Loss: 0.2979\n",
            "Epoch [227/300], Step [300/782], Loss: 0.2862\n",
            "Epoch [227/300], Step [400/782], Loss: 0.3924\n",
            "Epoch [227/300], Step [500/782], Loss: 0.2463\n",
            "Epoch [227/300], Step [600/782], Loss: 0.3223\n",
            "Epoch [227/300], Step [700/782], Loss: 0.4107\n",
            "Epoch [228/300], Step [100/782], Loss: 0.1553\n",
            "Epoch [228/300], Step [200/782], Loss: 0.2640\n",
            "Epoch [228/300], Step [300/782], Loss: 0.2409\n",
            "Epoch [228/300], Step [400/782], Loss: 0.3248\n",
            "Epoch [228/300], Step [500/782], Loss: 0.2694\n",
            "Epoch [228/300], Step [600/782], Loss: 0.4278\n",
            "Epoch [228/300], Step [700/782], Loss: 0.2892\n",
            "Epoch [229/300], Step [100/782], Loss: 0.1762\n",
            "Epoch [229/300], Step [200/782], Loss: 0.2723\n",
            "Epoch [229/300], Step [300/782], Loss: 0.3617\n",
            "Epoch [229/300], Step [400/782], Loss: 0.3236\n",
            "Epoch [229/300], Step [500/782], Loss: 0.2908\n",
            "Epoch [229/300], Step [600/782], Loss: 0.2233\n",
            "Epoch [229/300], Step [700/782], Loss: 0.1546\n",
            "Epoch [230/300], Step [100/782], Loss: 0.2527\n",
            "Epoch [230/300], Step [200/782], Loss: 0.3010\n",
            "Epoch [230/300], Step [300/782], Loss: 0.2116\n",
            "Epoch [230/300], Step [400/782], Loss: 0.2362\n",
            "Epoch [230/300], Step [500/782], Loss: 0.3190\n",
            "Epoch [230/300], Step [600/782], Loss: 0.2650\n",
            "Epoch [230/300], Step [700/782], Loss: 0.2231\n",
            "Epoch [231/300], Step [100/782], Loss: 0.3022\n",
            "Epoch [231/300], Step [200/782], Loss: 0.1541\n",
            "Epoch [231/300], Step [300/782], Loss: 0.3459\n",
            "Epoch [231/300], Step [400/782], Loss: 0.2591\n",
            "Epoch [231/300], Step [500/782], Loss: 0.2363\n",
            "Epoch [231/300], Step [600/782], Loss: 0.2232\n",
            "Epoch [231/300], Step [700/782], Loss: 0.2091\n",
            "Epoch [232/300], Step [100/782], Loss: 0.1443\n",
            "Epoch [232/300], Step [200/782], Loss: 0.1695\n",
            "Epoch [232/300], Step [300/782], Loss: 0.4139\n",
            "Epoch [232/300], Step [400/782], Loss: 0.2013\n",
            "Epoch [232/300], Step [500/782], Loss: 0.2316\n",
            "Epoch [232/300], Step [600/782], Loss: 0.2217\n",
            "Epoch [232/300], Step [700/782], Loss: 0.2909\n",
            "Epoch [233/300], Step [100/782], Loss: 0.1893\n",
            "Epoch [233/300], Step [200/782], Loss: 0.2336\n",
            "Epoch [233/300], Step [300/782], Loss: 0.1750\n",
            "Epoch [233/300], Step [400/782], Loss: 0.2811\n",
            "Epoch [233/300], Step [500/782], Loss: 0.2335\n",
            "Epoch [233/300], Step [600/782], Loss: 0.2962\n",
            "Epoch [233/300], Step [700/782], Loss: 0.2220\n",
            "Epoch [234/300], Step [100/782], Loss: 0.1989\n",
            "Epoch [234/300], Step [200/782], Loss: 0.1731\n",
            "Epoch [234/300], Step [300/782], Loss: 0.1604\n",
            "Epoch [234/300], Step [400/782], Loss: 0.2270\n",
            "Epoch [234/300], Step [500/782], Loss: 0.2209\n",
            "Epoch [234/300], Step [600/782], Loss: 0.2171\n",
            "Epoch [234/300], Step [700/782], Loss: 0.2816\n",
            "Epoch [235/300], Step [100/782], Loss: 0.2345\n",
            "Epoch [235/300], Step [200/782], Loss: 0.2268\n",
            "Epoch [235/300], Step [300/782], Loss: 0.2060\n",
            "Epoch [235/300], Step [400/782], Loss: 0.1913\n",
            "Epoch [235/300], Step [500/782], Loss: 0.1685\n",
            "Epoch [235/300], Step [600/782], Loss: 0.1460\n",
            "Epoch [235/300], Step [700/782], Loss: 0.2042\n",
            "Epoch [236/300], Step [100/782], Loss: 0.2062\n",
            "Epoch [236/300], Step [200/782], Loss: 0.2817\n",
            "Epoch [236/300], Step [300/782], Loss: 0.2374\n",
            "Epoch [236/300], Step [400/782], Loss: 0.1454\n",
            "Epoch [236/300], Step [500/782], Loss: 0.1571\n",
            "Epoch [236/300], Step [600/782], Loss: 0.3322\n",
            "Epoch [236/300], Step [700/782], Loss: 0.3048\n",
            "Epoch [237/300], Step [100/782], Loss: 0.2876\n",
            "Epoch [237/300], Step [200/782], Loss: 0.1917\n",
            "Epoch [237/300], Step [300/782], Loss: 0.1368\n",
            "Epoch [237/300], Step [400/782], Loss: 0.1273\n",
            "Epoch [237/300], Step [500/782], Loss: 0.1643\n",
            "Epoch [237/300], Step [600/782], Loss: 0.1847\n",
            "Epoch [237/300], Step [700/782], Loss: 0.1939\n",
            "Epoch [238/300], Step [100/782], Loss: 0.1288\n",
            "Epoch [238/300], Step [200/782], Loss: 0.2042\n",
            "Epoch [238/300], Step [300/782], Loss: 0.2300\n",
            "Epoch [238/300], Step [400/782], Loss: 0.1631\n",
            "Epoch [238/300], Step [500/782], Loss: 0.2343\n",
            "Epoch [238/300], Step [600/782], Loss: 0.1346\n",
            "Epoch [238/300], Step [700/782], Loss: 0.2145\n",
            "Epoch [239/300], Step [100/782], Loss: 0.2080\n",
            "Epoch [239/300], Step [200/782], Loss: 0.2001\n",
            "Epoch [239/300], Step [300/782], Loss: 0.1895\n",
            "Epoch [239/300], Step [400/782], Loss: 0.1787\n",
            "Epoch [239/300], Step [500/782], Loss: 0.2996\n",
            "Epoch [239/300], Step [600/782], Loss: 0.1741\n",
            "Epoch [239/300], Step [700/782], Loss: 0.1988\n",
            "Epoch [240/300], Step [100/782], Loss: 0.2300\n",
            "Epoch [240/300], Step [200/782], Loss: 0.1886\n",
            "Epoch [240/300], Step [300/782], Loss: 0.2367\n",
            "Epoch [240/300], Step [400/782], Loss: 0.1812\n",
            "Epoch [240/300], Step [500/782], Loss: 0.2353\n",
            "Epoch [240/300], Step [600/782], Loss: 0.3356\n",
            "Epoch [240/300], Step [700/782], Loss: 0.1768\n",
            "Epoch [241/300], Step [100/782], Loss: 0.1544\n",
            "Epoch [241/300], Step [200/782], Loss: 0.1922\n",
            "Epoch [241/300], Step [300/782], Loss: 0.2081\n",
            "Epoch [241/300], Step [400/782], Loss: 0.2159\n",
            "Epoch [241/300], Step [500/782], Loss: 0.1875\n",
            "Epoch [241/300], Step [600/782], Loss: 0.1761\n",
            "Epoch [241/300], Step [700/782], Loss: 0.1889\n",
            "Epoch [242/300], Step [100/782], Loss: 0.1997\n",
            "Epoch [242/300], Step [200/782], Loss: 0.2250\n",
            "Epoch [242/300], Step [300/782], Loss: 0.1210\n",
            "Epoch [242/300], Step [400/782], Loss: 0.1577\n",
            "Epoch [242/300], Step [500/782], Loss: 0.2621\n",
            "Epoch [242/300], Step [600/782], Loss: 0.2823\n",
            "Epoch [242/300], Step [700/782], Loss: 0.1575\n",
            "Epoch [243/300], Step [100/782], Loss: 0.1519\n",
            "Epoch [243/300], Step [200/782], Loss: 0.1387\n",
            "Epoch [243/300], Step [300/782], Loss: 0.2301\n",
            "Epoch [243/300], Step [400/782], Loss: 0.1851\n",
            "Epoch [243/300], Step [500/782], Loss: 0.2303\n",
            "Epoch [243/300], Step [600/782], Loss: 0.1571\n",
            "Epoch [243/300], Step [700/782], Loss: 0.1207\n",
            "Epoch [244/300], Step [100/782], Loss: 0.1160\n",
            "Epoch [244/300], Step [200/782], Loss: 0.1839\n",
            "Epoch [244/300], Step [300/782], Loss: 0.1960\n",
            "Epoch [244/300], Step [400/782], Loss: 0.1445\n",
            "Epoch [244/300], Step [500/782], Loss: 0.1972\n",
            "Epoch [244/300], Step [600/782], Loss: 0.3335\n",
            "Epoch [244/300], Step [700/782], Loss: 0.2137\n",
            "Epoch [245/300], Step [100/782], Loss: 0.1358\n",
            "Epoch [245/300], Step [200/782], Loss: 0.1619\n",
            "Epoch [245/300], Step [300/782], Loss: 0.1678\n",
            "Epoch [245/300], Step [400/782], Loss: 0.2283\n",
            "Epoch [245/300], Step [500/782], Loss: 0.2483\n",
            "Epoch [245/300], Step [600/782], Loss: 0.2754\n",
            "Epoch [245/300], Step [700/782], Loss: 0.1653\n",
            "Epoch [246/300], Step [100/782], Loss: 0.2104\n",
            "Epoch [246/300], Step [200/782], Loss: 0.1302\n",
            "Epoch [246/300], Step [300/782], Loss: 0.1605\n",
            "Epoch [246/300], Step [400/782], Loss: 0.1856\n",
            "Epoch [246/300], Step [500/782], Loss: 0.2413\n",
            "Epoch [246/300], Step [600/782], Loss: 0.1015\n",
            "Epoch [246/300], Step [700/782], Loss: 0.1881\n",
            "Epoch [247/300], Step [100/782], Loss: 0.1813\n",
            "Epoch [247/300], Step [200/782], Loss: 0.1841\n",
            "Epoch [247/300], Step [300/782], Loss: 0.2678\n",
            "Epoch [247/300], Step [400/782], Loss: 0.1971\n",
            "Epoch [247/300], Step [500/782], Loss: 0.1156\n",
            "Epoch [247/300], Step [600/782], Loss: 0.1519\n",
            "Epoch [247/300], Step [700/782], Loss: 0.1310\n",
            "Epoch [248/300], Step [100/782], Loss: 0.1997\n",
            "Epoch [248/300], Step [200/782], Loss: 0.1638\n",
            "Epoch [248/300], Step [300/782], Loss: 0.1286\n",
            "Epoch [248/300], Step [400/782], Loss: 0.1754\n",
            "Epoch [248/300], Step [500/782], Loss: 0.1633\n",
            "Epoch [248/300], Step [600/782], Loss: 0.1316\n",
            "Epoch [248/300], Step [700/782], Loss: 0.1726\n",
            "Epoch [249/300], Step [100/782], Loss: 0.1405\n",
            "Epoch [249/300], Step [200/782], Loss: 0.1328\n",
            "Epoch [249/300], Step [300/782], Loss: 0.1189\n",
            "Epoch [249/300], Step [400/782], Loss: 0.1756\n",
            "Epoch [249/300], Step [500/782], Loss: 0.1612\n",
            "Epoch [249/300], Step [600/782], Loss: 0.1744\n",
            "Epoch [249/300], Step [700/782], Loss: 0.2366\n",
            "Epoch [250/300], Step [100/782], Loss: 0.1384\n",
            "Epoch [250/300], Step [200/782], Loss: 0.1625\n",
            "Epoch [250/300], Step [300/782], Loss: 0.2425\n",
            "Epoch [250/300], Step [400/782], Loss: 0.1636\n",
            "Epoch [250/300], Step [500/782], Loss: 0.1907\n",
            "Epoch [250/300], Step [600/782], Loss: 0.1516\n",
            "Epoch [250/300], Step [700/782], Loss: 0.1424\n",
            "Epoch [251/300], Step [100/782], Loss: 0.1510\n",
            "Epoch [251/300], Step [200/782], Loss: 0.1435\n",
            "Epoch [251/300], Step [300/782], Loss: 0.1370\n",
            "Epoch [251/300], Step [400/782], Loss: 0.1400\n",
            "Epoch [251/300], Step [500/782], Loss: 0.2075\n",
            "Epoch [251/300], Step [600/782], Loss: 0.1456\n",
            "Epoch [251/300], Step [700/782], Loss: 0.1777\n",
            "Epoch [252/300], Step [100/782], Loss: 0.1409\n",
            "Epoch [252/300], Step [200/782], Loss: 0.1663\n",
            "Epoch [252/300], Step [300/782], Loss: 0.1364\n",
            "Epoch [252/300], Step [400/782], Loss: 0.2232\n",
            "Epoch [252/300], Step [500/782], Loss: 0.2385\n",
            "Epoch [252/300], Step [600/782], Loss: 0.1046\n",
            "Epoch [252/300], Step [700/782], Loss: 0.1036\n",
            "Epoch [253/300], Step [100/782], Loss: 0.1655\n",
            "Epoch [253/300], Step [200/782], Loss: 0.2117\n",
            "Epoch [253/300], Step [300/782], Loss: 0.1621\n",
            "Epoch [253/300], Step [400/782], Loss: 0.2133\n",
            "Epoch [253/300], Step [500/782], Loss: 0.1222\n",
            "Epoch [253/300], Step [600/782], Loss: 0.1122\n",
            "Epoch [253/300], Step [700/782], Loss: 0.1794\n",
            "Epoch [254/300], Step [100/782], Loss: 0.0772\n",
            "Epoch [254/300], Step [200/782], Loss: 0.0949\n",
            "Epoch [254/300], Step [300/782], Loss: 0.2016\n",
            "Epoch [254/300], Step [400/782], Loss: 0.1184\n",
            "Epoch [254/300], Step [500/782], Loss: 0.1191\n",
            "Epoch [254/300], Step [600/782], Loss: 0.1706\n",
            "Epoch [254/300], Step [700/782], Loss: 0.1100\n",
            "Epoch [255/300], Step [100/782], Loss: 0.1563\n",
            "Epoch [255/300], Step [200/782], Loss: 0.1320\n",
            "Epoch [255/300], Step [300/782], Loss: 0.1589\n",
            "Epoch [255/300], Step [400/782], Loss: 0.1980\n",
            "Epoch [255/300], Step [500/782], Loss: 0.1517\n",
            "Epoch [255/300], Step [600/782], Loss: 0.1631\n",
            "Epoch [255/300], Step [700/782], Loss: 0.1711\n",
            "Epoch [256/300], Step [100/782], Loss: 0.2501\n",
            "Epoch [256/300], Step [200/782], Loss: 0.1776\n",
            "Epoch [256/300], Step [300/782], Loss: 0.1040\n",
            "Epoch [256/300], Step [400/782], Loss: 0.1575\n",
            "Epoch [256/300], Step [500/782], Loss: 0.1374\n",
            "Epoch [256/300], Step [600/782], Loss: 0.2291\n",
            "Epoch [256/300], Step [700/782], Loss: 0.1788\n",
            "Epoch [257/300], Step [100/782], Loss: 0.1381\n",
            "Epoch [257/300], Step [200/782], Loss: 0.1592\n",
            "Epoch [257/300], Step [300/782], Loss: 0.1237\n",
            "Epoch [257/300], Step [400/782], Loss: 0.1920\n",
            "Epoch [257/300], Step [500/782], Loss: 0.1589\n",
            "Epoch [257/300], Step [600/782], Loss: 0.1314\n",
            "Epoch [257/300], Step [700/782], Loss: 0.1308\n",
            "Epoch [258/300], Step [100/782], Loss: 0.1143\n",
            "Epoch [258/300], Step [200/782], Loss: 0.1221\n",
            "Epoch [258/300], Step [300/782], Loss: 0.7529\n",
            "Epoch [258/300], Step [400/782], Loss: 0.1464\n",
            "Epoch [258/300], Step [500/782], Loss: 0.1704\n",
            "Epoch [258/300], Step [600/782], Loss: 0.1321\n",
            "Epoch [258/300], Step [700/782], Loss: 0.1344\n",
            "Epoch [259/300], Step [100/782], Loss: 0.1205\n",
            "Epoch [259/300], Step [200/782], Loss: 0.1179\n",
            "Epoch [259/300], Step [300/782], Loss: 0.1469\n",
            "Epoch [259/300], Step [400/782], Loss: 0.1723\n",
            "Epoch [259/300], Step [500/782], Loss: 0.1894\n",
            "Epoch [259/300], Step [600/782], Loss: 0.1339\n",
            "Epoch [259/300], Step [700/782], Loss: 0.1389\n",
            "Epoch [260/300], Step [100/782], Loss: 0.3284\n",
            "Epoch [260/300], Step [200/782], Loss: 0.1729\n",
            "Epoch [260/300], Step [300/782], Loss: 0.1316\n",
            "Epoch [260/300], Step [400/782], Loss: 0.1435\n",
            "Epoch [260/300], Step [500/782], Loss: 0.1198\n",
            "Epoch [260/300], Step [600/782], Loss: 0.0781\n",
            "Epoch [260/300], Step [700/782], Loss: 0.1774\n",
            "Epoch [261/300], Step [100/782], Loss: 0.1515\n",
            "Epoch [261/300], Step [200/782], Loss: 0.1958\n",
            "Epoch [261/300], Step [300/782], Loss: 0.1038\n",
            "Epoch [261/300], Step [400/782], Loss: 0.1354\n",
            "Epoch [261/300], Step [500/782], Loss: 0.0782\n",
            "Epoch [261/300], Step [600/782], Loss: 0.1241\n",
            "Epoch [261/300], Step [700/782], Loss: 0.1853\n",
            "Epoch [262/300], Step [100/782], Loss: 0.0824\n",
            "Epoch [262/300], Step [200/782], Loss: 0.1346\n",
            "Epoch [262/300], Step [300/782], Loss: 0.3231\n",
            "Epoch [262/300], Step [400/782], Loss: 0.1615\n",
            "Epoch [262/300], Step [500/782], Loss: 0.1372\n",
            "Epoch [262/300], Step [600/782], Loss: 0.1486\n",
            "Epoch [262/300], Step [700/782], Loss: 0.2167\n",
            "Epoch [263/300], Step [100/782], Loss: 0.0830\n",
            "Epoch [263/300], Step [200/782], Loss: 0.1490\n",
            "Epoch [263/300], Step [300/782], Loss: 0.1948\n",
            "Epoch [263/300], Step [400/782], Loss: 0.1431\n",
            "Epoch [263/300], Step [500/782], Loss: 0.1280\n",
            "Epoch [263/300], Step [600/782], Loss: 0.2300\n",
            "Epoch [263/300], Step [700/782], Loss: 0.0873\n",
            "Epoch [264/300], Step [100/782], Loss: 0.1555\n",
            "Epoch [264/300], Step [200/782], Loss: 0.1150\n",
            "Epoch [264/300], Step [300/782], Loss: 0.1010\n",
            "Epoch [264/300], Step [400/782], Loss: 0.1245\n",
            "Epoch [264/300], Step [500/782], Loss: 0.0888\n",
            "Epoch [264/300], Step [600/782], Loss: 0.1146\n",
            "Epoch [264/300], Step [700/782], Loss: 0.1018\n",
            "Epoch [265/300], Step [100/782], Loss: 0.1035\n",
            "Epoch [265/300], Step [200/782], Loss: 0.0980\n",
            "Epoch [265/300], Step [300/782], Loss: 0.1202\n",
            "Epoch [265/300], Step [400/782], Loss: 0.1933\n",
            "Epoch [265/300], Step [500/782], Loss: 0.1416\n",
            "Epoch [265/300], Step [600/782], Loss: 0.0918\n",
            "Epoch [265/300], Step [700/782], Loss: 0.1405\n",
            "Epoch [266/300], Step [100/782], Loss: 0.0929\n",
            "Epoch [266/300], Step [200/782], Loss: 0.1398\n",
            "Epoch [266/300], Step [300/782], Loss: 0.1401\n",
            "Epoch [266/300], Step [400/782], Loss: 0.1183\n",
            "Epoch [266/300], Step [500/782], Loss: 0.1269\n",
            "Epoch [266/300], Step [600/782], Loss: 0.1326\n",
            "Epoch [266/300], Step [700/782], Loss: 0.0799\n",
            "Epoch [267/300], Step [100/782], Loss: 0.0904\n",
            "Epoch [267/300], Step [200/782], Loss: 0.0862\n",
            "Epoch [267/300], Step [300/782], Loss: 0.1547\n",
            "Epoch [267/300], Step [400/782], Loss: 0.1216\n",
            "Epoch [267/300], Step [500/782], Loss: 0.1047\n",
            "Epoch [267/300], Step [600/782], Loss: 0.1280\n",
            "Epoch [267/300], Step [700/782], Loss: 0.1126\n",
            "Epoch [268/300], Step [100/782], Loss: 0.1062\n",
            "Epoch [268/300], Step [200/782], Loss: 0.1535\n",
            "Epoch [268/300], Step [300/782], Loss: 0.1533\n",
            "Epoch [268/300], Step [400/782], Loss: 0.0991\n",
            "Epoch [268/300], Step [500/782], Loss: 0.0889\n",
            "Epoch [268/300], Step [600/782], Loss: 0.1013\n",
            "Epoch [268/300], Step [700/782], Loss: 0.2817\n",
            "Epoch [269/300], Step [100/782], Loss: 0.1164\n",
            "Epoch [269/300], Step [200/782], Loss: 0.1414\n",
            "Epoch [269/300], Step [300/782], Loss: 0.0824\n",
            "Epoch [269/300], Step [400/782], Loss: 0.1187\n",
            "Epoch [269/300], Step [500/782], Loss: 0.0800\n",
            "Epoch [269/300], Step [600/782], Loss: 0.1217\n",
            "Epoch [269/300], Step [700/782], Loss: 0.0873\n",
            "Epoch [270/300], Step [100/782], Loss: 0.0981\n",
            "Epoch [270/300], Step [200/782], Loss: 0.1048\n",
            "Epoch [270/300], Step [300/782], Loss: 0.1268\n",
            "Epoch [270/300], Step [400/782], Loss: 0.1008\n",
            "Epoch [270/300], Step [500/782], Loss: 0.1440\n",
            "Epoch [270/300], Step [600/782], Loss: 0.0867\n",
            "Epoch [270/300], Step [700/782], Loss: 0.1036\n",
            "Epoch [271/300], Step [100/782], Loss: 0.1062\n",
            "Epoch [271/300], Step [200/782], Loss: 0.1258\n",
            "Epoch [271/300], Step [300/782], Loss: 0.0748\n",
            "Epoch [271/300], Step [400/782], Loss: 0.0970\n",
            "Epoch [271/300], Step [500/782], Loss: 0.1047\n",
            "Epoch [271/300], Step [600/782], Loss: 0.1241\n",
            "Epoch [271/300], Step [700/782], Loss: 0.1335\n",
            "Epoch [272/300], Step [100/782], Loss: 0.0914\n",
            "Epoch [272/300], Step [200/782], Loss: 0.1570\n",
            "Epoch [272/300], Step [300/782], Loss: 0.0779\n",
            "Epoch [272/300], Step [400/782], Loss: 0.2144\n",
            "Epoch [272/300], Step [500/782], Loss: 0.1667\n",
            "Epoch [272/300], Step [600/782], Loss: 0.1538\n",
            "Epoch [272/300], Step [700/782], Loss: 0.1829\n",
            "Epoch [273/300], Step [100/782], Loss: 0.0982\n",
            "Epoch [273/300], Step [200/782], Loss: 0.0881\n",
            "Epoch [273/300], Step [300/782], Loss: 0.0701\n",
            "Epoch [273/300], Step [400/782], Loss: 0.1038\n",
            "Epoch [273/300], Step [500/782], Loss: 0.1430\n",
            "Epoch [273/300], Step [600/782], Loss: 0.1376\n",
            "Epoch [273/300], Step [700/782], Loss: 0.1076\n",
            "Epoch [274/300], Step [100/782], Loss: 0.0916\n",
            "Epoch [274/300], Step [200/782], Loss: 0.0876\n",
            "Epoch [274/300], Step [300/782], Loss: 0.0830\n",
            "Epoch [274/300], Step [400/782], Loss: 0.1975\n",
            "Epoch [274/300], Step [500/782], Loss: 0.0701\n",
            "Epoch [274/300], Step [600/782], Loss: 0.0992\n",
            "Epoch [274/300], Step [700/782], Loss: 0.2164\n",
            "Epoch [275/300], Step [100/782], Loss: 0.1146\n",
            "Epoch [275/300], Step [200/782], Loss: 0.1230\n",
            "Epoch [275/300], Step [300/782], Loss: 0.1523\n",
            "Epoch [275/300], Step [400/782], Loss: 0.1388\n",
            "Epoch [275/300], Step [500/782], Loss: 0.0937\n",
            "Epoch [275/300], Step [600/782], Loss: 0.1448\n",
            "Epoch [275/300], Step [700/782], Loss: 0.0689\n",
            "Epoch [276/300], Step [100/782], Loss: 0.0658\n",
            "Epoch [276/300], Step [200/782], Loss: 0.1182\n",
            "Epoch [276/300], Step [300/782], Loss: 0.1328\n",
            "Epoch [276/300], Step [400/782], Loss: 0.0931\n",
            "Epoch [276/300], Step [500/782], Loss: 0.1113\n",
            "Epoch [276/300], Step [600/782], Loss: 0.2710\n",
            "Epoch [276/300], Step [700/782], Loss: 0.0753\n",
            "Epoch [277/300], Step [100/782], Loss: 0.1002\n",
            "Epoch [277/300], Step [200/782], Loss: 0.0946\n",
            "Epoch [277/300], Step [300/782], Loss: 0.0885\n",
            "Epoch [277/300], Step [400/782], Loss: 0.0668\n",
            "Epoch [277/300], Step [500/782], Loss: 0.1175\n",
            "Epoch [277/300], Step [600/782], Loss: 0.1164\n",
            "Epoch [277/300], Step [700/782], Loss: 0.1216\n",
            "Epoch [278/300], Step [100/782], Loss: 0.0739\n",
            "Epoch [278/300], Step [200/782], Loss: 0.0866\n",
            "Epoch [278/300], Step [300/782], Loss: 0.0961\n",
            "Epoch [278/300], Step [400/782], Loss: 0.1020\n",
            "Epoch [278/300], Step [500/782], Loss: 0.0973\n",
            "Epoch [278/300], Step [600/782], Loss: 0.1165\n",
            "Epoch [278/300], Step [700/782], Loss: 0.1117\n",
            "Epoch [279/300], Step [100/782], Loss: 0.1081\n",
            "Epoch [279/300], Step [200/782], Loss: 0.0626\n",
            "Epoch [279/300], Step [300/782], Loss: 0.1087\n",
            "Epoch [279/300], Step [400/782], Loss: 0.0918\n",
            "Epoch [279/300], Step [500/782], Loss: 0.0999\n",
            "Epoch [279/300], Step [600/782], Loss: 0.0856\n",
            "Epoch [279/300], Step [700/782], Loss: 0.0695\n",
            "Epoch [280/300], Step [100/782], Loss: 0.0608\n",
            "Epoch [280/300], Step [200/782], Loss: 0.0641\n",
            "Epoch [280/300], Step [300/782], Loss: 0.0706\n",
            "Epoch [280/300], Step [400/782], Loss: 0.1223\n",
            "Epoch [280/300], Step [500/782], Loss: 0.0734\n",
            "Epoch [280/300], Step [600/782], Loss: 0.0932\n",
            "Epoch [280/300], Step [700/782], Loss: 0.2685\n",
            "Epoch [281/300], Step [100/782], Loss: 0.0918\n",
            "Epoch [281/300], Step [200/782], Loss: 0.1774\n",
            "Epoch [281/300], Step [300/782], Loss: 0.0738\n",
            "Epoch [281/300], Step [400/782], Loss: 0.1381\n",
            "Epoch [281/300], Step [500/782], Loss: 0.1813\n",
            "Epoch [281/300], Step [600/782], Loss: 0.0645\n",
            "Epoch [281/300], Step [700/782], Loss: 0.0620\n",
            "Epoch [282/300], Step [100/782], Loss: 0.0703\n",
            "Epoch [282/300], Step [200/782], Loss: 0.1386\n",
            "Epoch [282/300], Step [300/782], Loss: 0.0478\n",
            "Epoch [282/300], Step [400/782], Loss: 0.0936\n",
            "Epoch [282/300], Step [500/782], Loss: 0.0766\n",
            "Epoch [282/300], Step [600/782], Loss: 0.1299\n",
            "Epoch [282/300], Step [700/782], Loss: 0.1072\n",
            "Epoch [283/300], Step [100/782], Loss: 0.1807\n",
            "Epoch [283/300], Step [200/782], Loss: 0.1018\n",
            "Epoch [283/300], Step [300/782], Loss: 0.0729\n",
            "Epoch [283/300], Step [400/782], Loss: 0.0584\n",
            "Epoch [283/300], Step [500/782], Loss: 0.0755\n",
            "Epoch [283/300], Step [600/782], Loss: 0.1247\n",
            "Epoch [283/300], Step [700/782], Loss: 0.1173\n",
            "Epoch [284/300], Step [100/782], Loss: 0.0773\n",
            "Epoch [284/300], Step [200/782], Loss: 2.3209\n",
            "Epoch [284/300], Step [300/782], Loss: 0.1213\n",
            "Epoch [284/300], Step [400/782], Loss: 0.0830\n",
            "Epoch [284/300], Step [500/782], Loss: 0.0974\n",
            "Epoch [284/300], Step [600/782], Loss: 0.0863\n",
            "Epoch [284/300], Step [700/782], Loss: 0.0682\n",
            "Epoch [285/300], Step [100/782], Loss: 0.0844\n",
            "Epoch [285/300], Step [200/782], Loss: 0.0864\n",
            "Epoch [285/300], Step [300/782], Loss: 0.0652\n",
            "Epoch [285/300], Step [400/782], Loss: 0.1396\n",
            "Epoch [285/300], Step [500/782], Loss: 0.0678\n",
            "Epoch [285/300], Step [600/782], Loss: 0.2237\n",
            "Epoch [285/300], Step [700/782], Loss: 0.0743\n",
            "Epoch [286/300], Step [100/782], Loss: 0.0585\n",
            "Epoch [286/300], Step [200/782], Loss: 0.0705\n",
            "Epoch [286/300], Step [300/782], Loss: 0.0793\n",
            "Epoch [286/300], Step [400/782], Loss: 0.0830\n",
            "Epoch [286/300], Step [500/782], Loss: 0.1061\n",
            "Epoch [286/300], Step [600/782], Loss: 0.0322\n",
            "Epoch [286/300], Step [700/782], Loss: 0.0947\n",
            "Epoch [287/300], Step [100/782], Loss: 0.0480\n",
            "Epoch [287/300], Step [200/782], Loss: 0.0667\n",
            "Epoch [287/300], Step [300/782], Loss: 0.0546\n",
            "Epoch [287/300], Step [400/782], Loss: 0.0531\n",
            "Epoch [287/300], Step [500/782], Loss: 0.0964\n",
            "Epoch [287/300], Step [600/782], Loss: 0.0898\n",
            "Epoch [287/300], Step [700/782], Loss: 0.0980\n",
            "Epoch [288/300], Step [100/782], Loss: 0.2351\n",
            "Epoch [288/300], Step [200/782], Loss: 0.0965\n",
            "Epoch [288/300], Step [300/782], Loss: 0.0920\n",
            "Epoch [288/300], Step [400/782], Loss: 0.0758\n",
            "Epoch [288/300], Step [500/782], Loss: 0.0721\n",
            "Epoch [288/300], Step [600/782], Loss: 0.0704\n",
            "Epoch [288/300], Step [700/782], Loss: 0.0398\n",
            "Epoch [289/300], Step [100/782], Loss: 0.0491\n",
            "Epoch [289/300], Step [200/782], Loss: 0.1124\n",
            "Epoch [289/300], Step [300/782], Loss: 0.0550\n",
            "Epoch [289/300], Step [400/782], Loss: 0.0514\n",
            "Epoch [289/300], Step [500/782], Loss: 0.1037\n",
            "Epoch [289/300], Step [600/782], Loss: 0.0855\n",
            "Epoch [289/300], Step [700/782], Loss: 0.0591\n",
            "Epoch [290/300], Step [100/782], Loss: 0.0678\n",
            "Epoch [290/300], Step [200/782], Loss: 0.0801\n",
            "Epoch [290/300], Step [300/782], Loss: 0.0813\n",
            "Epoch [290/300], Step [400/782], Loss: 0.0470\n",
            "Epoch [290/300], Step [500/782], Loss: 0.0864\n",
            "Epoch [290/300], Step [600/782], Loss: 0.0505\n",
            "Epoch [290/300], Step [700/782], Loss: 0.0441\n",
            "Epoch [291/300], Step [100/782], Loss: 0.0593\n",
            "Epoch [291/300], Step [200/782], Loss: 0.0727\n",
            "Epoch [291/300], Step [300/782], Loss: 0.1148\n",
            "Epoch [291/300], Step [400/782], Loss: 0.0746\n",
            "Epoch [291/300], Step [500/782], Loss: 0.0482\n",
            "Epoch [291/300], Step [600/782], Loss: 0.0648\n",
            "Epoch [291/300], Step [700/782], Loss: 0.0698\n",
            "Epoch [292/300], Step [100/782], Loss: 0.0754\n",
            "Epoch [292/300], Step [200/782], Loss: 0.0881\n",
            "Epoch [292/300], Step [300/782], Loss: 0.0619\n",
            "Epoch [292/300], Step [400/782], Loss: 0.0603\n",
            "Epoch [292/300], Step [500/782], Loss: 0.0420\n",
            "Epoch [292/300], Step [600/782], Loss: 0.0706\n",
            "Epoch [292/300], Step [700/782], Loss: 0.0641\n",
            "Epoch [293/300], Step [100/782], Loss: 0.0499\n",
            "Epoch [293/300], Step [200/782], Loss: 0.0523\n",
            "Epoch [293/300], Step [300/782], Loss: 0.0741\n",
            "Epoch [293/300], Step [400/782], Loss: 0.0464\n",
            "Epoch [293/300], Step [500/782], Loss: 0.1022\n",
            "Epoch [293/300], Step [600/782], Loss: 0.1260\n",
            "Epoch [293/300], Step [700/782], Loss: 0.0568\n",
            "Epoch [294/300], Step [100/782], Loss: 0.1078\n",
            "Epoch [294/300], Step [200/782], Loss: 0.0704\n",
            "Epoch [294/300], Step [300/782], Loss: 0.0642\n",
            "Epoch [294/300], Step [400/782], Loss: 0.1102\n",
            "Epoch [294/300], Step [500/782], Loss: 0.0409\n",
            "Epoch [294/300], Step [600/782], Loss: 0.0344\n",
            "Epoch [294/300], Step [700/782], Loss: 0.1237\n",
            "Epoch [295/300], Step [100/782], Loss: 0.0444\n",
            "Epoch [295/300], Step [200/782], Loss: 0.0461\n",
            "Epoch [295/300], Step [300/782], Loss: 0.0666\n",
            "Epoch [295/300], Step [400/782], Loss: 0.0581\n",
            "Epoch [295/300], Step [500/782], Loss: 0.0650\n",
            "Epoch [295/300], Step [600/782], Loss: 0.0744\n",
            "Epoch [295/300], Step [700/782], Loss: 0.1118\n",
            "Epoch [296/300], Step [100/782], Loss: 0.0524\n",
            "Epoch [296/300], Step [200/782], Loss: 0.0565\n",
            "Epoch [296/300], Step [300/782], Loss: 0.0407\n",
            "Epoch [296/300], Step [400/782], Loss: 0.0832\n",
            "Epoch [296/300], Step [500/782], Loss: 0.0503\n",
            "Epoch [296/300], Step [600/782], Loss: 0.0548\n",
            "Epoch [296/300], Step [700/782], Loss: 0.0404\n",
            "Epoch [297/300], Step [100/782], Loss: 0.0513\n",
            "Epoch [297/300], Step [200/782], Loss: 0.0565\n",
            "Epoch [297/300], Step [300/782], Loss: 0.1262\n",
            "Epoch [297/300], Step [400/782], Loss: 0.0444\n",
            "Epoch [297/300], Step [500/782], Loss: 0.1031\n",
            "Epoch [297/300], Step [600/782], Loss: 0.0724\n",
            "Epoch [297/300], Step [700/782], Loss: 0.0686\n",
            "Epoch [298/300], Step [100/782], Loss: 0.0433\n",
            "Epoch [298/300], Step [200/782], Loss: 0.0420\n",
            "Epoch [298/300], Step [300/782], Loss: 0.0498\n",
            "Epoch [298/300], Step [400/782], Loss: 0.0547\n",
            "Epoch [298/300], Step [500/782], Loss: 0.0452\n",
            "Epoch [298/300], Step [600/782], Loss: 0.0559\n",
            "Epoch [298/300], Step [700/782], Loss: 0.0596\n",
            "Epoch [299/300], Step [100/782], Loss: 0.0390\n",
            "Epoch [299/300], Step [200/782], Loss: 0.0617\n",
            "Epoch [299/300], Step [300/782], Loss: 0.0791\n",
            "Epoch [299/300], Step [400/782], Loss: 0.0298\n",
            "Epoch [299/300], Step [500/782], Loss: 0.0326\n",
            "Epoch [299/300], Step [600/782], Loss: 0.0670\n",
            "Epoch [299/300], Step [700/782], Loss: 0.0431\n",
            "Epoch [300/300], Step [100/782], Loss: 0.0501\n",
            "Epoch [300/300], Step [200/782], Loss: 0.0614\n",
            "Epoch [300/300], Step [300/782], Loss: 0.0446\n",
            "Epoch [300/300], Step [400/782], Loss: 0.0393\n",
            "Epoch [300/300], Step [500/782], Loss: 0.0378\n",
            "Epoch [300/300], Step [600/782], Loss: 0.0672\n",
            "Epoch [300/300], Step [700/782], Loss: 0.0566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.reshape(-1, input_size)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy on the {} training images: {} %'.format(total, 100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'extended_nn.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsKjaOZF01u2",
        "outputId": "224613f0-7fb7-4811-d1f5-fea11e9d0551"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the 50000 training images: 99.696 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss for 1a. = 1.8256\n",
        "\n",
        "Loss for 1b. = 0.0566\n",
        "\n",
        "Acurracy for 3 hidden layer = 99.696%\n",
        "\n",
        "\n",
        "Data set fits into the 3hidden layer model well with low losses and high Accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "aVqLfc8OWmK9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}